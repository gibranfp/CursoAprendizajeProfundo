{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1b_perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gibranfp/CursoAprendizajeProfundo/blob/master/notebooks/1b_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8X1X5KGWhZT"
      },
      "source": [
        "# Neuronas artificiales \n",
        "---\n",
        "La neurona artificial es un modelo simplificado de la neurona natural, la cual trata de imitar 3 aspectos principales: \n",
        "\n",
        "1. La fuerza sináptica que pondera los impulsos recibidos\n",
        "2. La acumulación de estos impulsos ponderados \n",
        "3. La activación de la neurona que produce un impulso de respuesta a su salida. \n",
        "\n",
        "La primera neurona artificial fue la llamada Unidad de Umbral Lineal, propuesta en 1943 por Warren McCulloch y Walter Pitts, la cual presupone que tanto los valores de los atributos de entrada como los valores de salida son binarios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4qjFKtZafzQ"
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ec2lh4iaj7J"
      },
      "source": [
        "## Unidad de umbral lineal\n",
        "La operación que lleva a cabo una neurona artificial está dada por la suma pesada evaluada en una función de activación $\\phi$.  Una de las primeras funciones de activación utilizadas fue la escalón unitario, definida como\n",
        "\n",
        "$\n",
        "\\phi(x) = \\begin{cases} 1, & \\text{si } x > 0\\\\0, & \\text{en caso contrario}\\end{cases}\n",
        "$\n",
        "\n",
        "Esta se puede llevar a cabo con la siguiente función de Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DF3X4nFS8ze"
      },
      "source": [
        "def escalon(z):\n",
        "    if z > 0.0:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHUpzFQLY8Y1"
      },
      "source": [
        "Por su parte, la suma pesada simplemente consiste en multiplicar cada entrada por su correspondiente peso y sumarle el sesgo. Esto lo podemos expresar como\n",
        "\n",
        "$\n",
        "z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\cdots + w_d \\cdot x_d + b \n",
        "$\n",
        "\n",
        "En su forma vectorial\n",
        "\n",
        "$\n",
        "z = \\mathbf{w}^T \\mathbf{x} + b\n",
        "$\n",
        "\n",
        "Para realizar esto en Python, podemos usar la función `dot` de NumPY de la siguiente manera `z = np.dot(w.T, x) + b`. Así, la operación de la neurona completa sería:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G61wzAPaT3r3"
      },
      "source": [
        "def neurona(x, w, b):\n",
        "  z = np.dot(w.T, x) + b\n",
        "  a = escalon(z)\n",
        "\n",
        "  return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-VjUcwrZpmW"
      },
      "source": [
        "### OR ($\\lor$)\n",
        "Esta neurona es capaz de aproximar el operador OR, cuya salida es 1 cuando al menos 1 de las 2 entradas es 1:\n",
        "\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ------------- |:-------------:| -----:|\n",
        "|0 |0 |0|\n",
        "|0 |1 |1|\n",
        "|1 |0 |1|\n",
        "|1 |1 |1|\n",
        "\n",
        "La neurona recibe 2 valores binarios como entrada y produce un valor binario como salida. Específicamente, la neurona calcularía\n",
        "\n",
        "$\n",
        "\\hat{y} = \\phi(w_1 \\cdot x_1 + w_2 \\cdot x_2 + b)\n",
        "$\n",
        "\n",
        "Para poder aproximar la operación OR es necesario encontrar los valores apropiados de $w_1$, $w_2$ y $b$. Una posible elección sería 10, 10 y -5 respectivamente. Verifiquemos estos valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE5LjZh9TWcF",
        "outputId": "169ebbf7-eb7f-4c05-f7e3-fc0f98892d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "X = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
        "w = np.array([10, 10]).T\n",
        "b = -5\n",
        "\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\ty_hat')\n",
        "print('-----------------------------')\n",
        "for i in range(X.shape[0]):\n",
        "  y_hat = neurona(X[i, :], w, b)\n",
        "  print('{0} \\t{1}\\t{2}'.format(X[i, 0], X[i, 1], y_hat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------\n",
            "x_1 \tx_2 \ty_hat\n",
            "-----------------------------\n",
            "0.0 \t0.0\t0.0\n",
            "0.0 \t1.0\t1.0\n",
            "1.0 \t0.0\t1.0\n",
            "1.0 \t1.0\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnqeydbxeavD"
      },
      "source": [
        "### AND ($\\land$)\n",
        "De forma similar, podemos aproximar la operación AND:\n",
        "\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ------------- |:-------------:| -----:|\n",
        "|0 |0 |0|\n",
        "|0 |1 |0|\n",
        "|1 |0 |0|\n",
        "|1 |1 |1|\n",
        "\n",
        "Nuevamente, debemos encontrar los valores apropiados para los pesos y el sesgo. Probemos con $w_1 = 10$, $w_2 = 10$ y $b = -15$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdcp_-oqTc75",
        "outputId": "a470ea98-ca6d-46d6-aee6-e2da961b603c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "w = np.array([10, 10]).T\n",
        "b = -15 \n",
        "\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\ty_hat')\n",
        "print('-----------------------------')\n",
        "for i in range(X.shape[0]):\n",
        "  y_hat = neurona(X[i, :], w, b)\n",
        "  print('{0} \\t{1}\\t{2}'.format(X[i, 0], X[i, 1], y_hat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------\n",
            "x_1 \tx_2 \ty_hat\n",
            "-----------------------------\n",
            "0.0 \t0.0\t0.0\n",
            "0.0 \t1.0\t0.0\n",
            "1.0 \t0.0\t0.0\n",
            "1.0 \t1.0\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJW4l3Gcx-j2"
      },
      "source": [
        "### NAND \n",
        "También podemos aproximar la negación de la operación AND (NAND):\n",
        "\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ------------- |:-------------:| -----:|\n",
        "|0 |0 |1|\n",
        "|0 |1 |1|\n",
        "|1 |0 |1|\n",
        "|1 |1 |0|\n",
        "\n",
        "En este caso, los valores para los pesos y el sesgo son $w_1 = -10$, $w_2 = -10$ y $b = 15$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJoT4krVyGEo",
        "outputId": "30541e00-00ca-432b-f58a-fb5f24190551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "w = np.array([-10, -10]).T\n",
        "b = 15 \n",
        "\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\ty_hat')\n",
        "print('-----------------------------')\n",
        "for i in range(X.shape[0]):\n",
        "  y_hat = neurona(X[i, :], w, b)\n",
        "  print('{0} \\t{1}\\t{2}'.format(X[i, 0], X[i, 1], y_hat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------\n",
            "x_1 \tx_2 \ty_hat\n",
            "-----------------------------\n",
            "0.0 \t0.0\t1.0\n",
            "0.0 \t1.0\t1.0\n",
            "1.0 \t0.0\t1.0\n",
            "1.0 \t1.0\t0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqXVXWhve5pR"
      },
      "source": [
        "## Algoritmo del perceptrón\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7kivF042Heh"
      },
      "source": [
        "def perceptron(X, y, n_epochs = 10):\n",
        "    w_new = np.zeros(X.shape[1])\n",
        "    b_new = 0\n",
        "    for i in range(n_epochs):\n",
        "        serr = 0.0\n",
        "        for j in range(X.shape[0]):\n",
        "            w_old = w_new\n",
        "            b_old = b_new\n",
        "            \n",
        "            y_hat = neurona(X[j], w_old, b_old)\n",
        "            error = y[j] - y_hat\n",
        "           \n",
        "            w_new = w_old + error * X[j]     \n",
        "            b_new = b_old + error\n",
        "           \n",
        "            serr += np.abs(error)\n",
        "        print(\"Epoch {0}: error = {1}\".format(i, serr / float(X.shape[0])))\n",
        "\n",
        "    return w_new, b_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8WH4x7afCKv"
      },
      "source": [
        "### Aprendiendo la operación OR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqukG0wL9DhC"
      },
      "source": [
        "Probemos el algoritmo del perceptrón para aprender la operación lógica OR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOFZdaie9D6n",
        "outputId": "9d30af14-2e5a-4fc4-dbac-1c4cdd88d989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "X = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
        "y_or = np.array([0., 1., 1., 1.]) \n",
        "\n",
        "w, b = perceptron(X, y_or)\n",
        "\n",
        "print('\\nw_1 = {0}, w_2 = {1}, b = {2}'.format(w[0], w[1], b))\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\t y\\ty_hat')\n",
        "print('-----------------------------')\n",
        "for i in range(X.shape[0]):\n",
        "  y_hat = neurona(X[i], w, b)\n",
        "  print('{0}\\t{1}\\t{2}\\t{3}'.format(X[i, 0], X[i, 1], y_or[i], y_hat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: error = 0.25\n",
            "Epoch 1: error = 0.5\n",
            "Epoch 2: error = 0.25\n",
            "Epoch 3: error = 0.0\n",
            "Epoch 4: error = 0.0\n",
            "Epoch 5: error = 0.0\n",
            "Epoch 6: error = 0.0\n",
            "Epoch 7: error = 0.0\n",
            "Epoch 8: error = 0.0\n",
            "Epoch 9: error = 0.0\n",
            "\n",
            "w_1 = 1.0, w_2 = 1.0, b = 0.0\n",
            "-----------------------------\n",
            "x_1 \tx_2 \t y\ty_hat\n",
            "-----------------------------\n",
            "0.0\t0.0\t0.0\t0.0\n",
            "0.0\t1.0\t1.0\t1.0\n",
            "1.0\t0.0\t1.0\t1.0\n",
            "1.0\t1.0\t1.0\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSinG4G6rSCF"
      },
      "source": [
        "### Aprendiendo la operación AND\n",
        "Ahora veamos qué ocurre si en lugar de la operación OR tratamos de aprender la operación AND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqyp83uorS_i",
        "outputId": "9612fe21-40e7-4b44-e35e-aed4d5ab3d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "y_and = np.array([0., 0., 0., 1.])\n",
        "w, b = perceptron(X, y_and)\n",
        "\n",
        "print('\\nw_1 = {0}, w_2 = {1}, b = {2}'.format(w[0], w[1], b))\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\t y\\ty_hat')\n",
        "print('-----------------------------')\n",
        "for i in range(X.shape[0]):\n",
        "  y_hat = neurona(X[i], w, b)\n",
        "  print('{0}\\t{1}\\t{2}\\t{3}'.format(X[i, 0], X[i, 1], y_and[i], y_hat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: error = 0.25\n",
            "Epoch 1: error = 0.75\n",
            "Epoch 2: error = 0.75\n",
            "Epoch 3: error = 0.5\n",
            "Epoch 4: error = 0.25\n",
            "Epoch 5: error = 0.0\n",
            "Epoch 6: error = 0.0\n",
            "Epoch 7: error = 0.0\n",
            "Epoch 8: error = 0.0\n",
            "Epoch 9: error = 0.0\n",
            "\n",
            "w_1 = 2.0, w_2 = 1.0, b = -2.0\n",
            "-----------------------------\n",
            "x_1 \tx_2 \t y\ty_hat\n",
            "-----------------------------\n",
            "0.0\t0.0\t0.0\t0.0\n",
            "0.0\t1.0\t0.0\t0.0\n",
            "1.0\t0.0\t0.0\t0.0\n",
            "1.0\t1.0\t1.0\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c406Vb4Rz2bm"
      },
      "source": [
        "## Aproximando funciones no lineales: XOR ($\\oplus$)\n",
        "Minsky y Papert mostraron que una neurona del tipo LTU no puede aproximar de forma precisa una función no lineal como la compuerta XOR ($\\oplus$):\n",
        "\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ------------- |:-------------:| -----:|\n",
        "|0 |0 |0|\n",
        "|0 |1 |1|\n",
        "|1 |0 |1|\n",
        "|1 |1 |0|\n",
        "\n",
        "Sin embargo, es posible aproximar este tipo  combinando múltiples LTU conectadas en red. Por ejemplo, es posible llevar a cabo la operación XOR con operaciones OR, AND y NAND:\n",
        "\n",
        "$\n",
        "\t  x_1 \\mathbin{\\oplus} x_2 = (x_1 \\lor x_2) \\land \\neg(x_1 \\land x_2)\n",
        "\t$  \n",
        "\n",
        "Esto lo llevamos a cabo con la siguiente función:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucf4fSl01XTA"
      },
      "source": [
        "def multicapa(x, W1, b1, W2, b2):\n",
        "  escv = np.vectorize(escalon)\n",
        "  a = escv(np.dot(W1.T, x) + b1)\n",
        "  return escv(np.dot(W2.T, a) + b2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbStOf7Qif1P"
      },
      "source": [
        "Encontrando los valores de pesos y sesgos adecuados, podemos usar esta función para aproximar la operación XOR. Ya hemos encontrado los pesos y sesgos para las operaciones OR, AND y NAND, por lo que podemos usar estas neuronas con sus correspondientes pesos y sesgos. La red tendría 2 neuronas conectadas a las entradas que realizan las operaciones OR ($w_{11}^{\\{1\\}} = 10$, $w_{12}^{\\{1\\}} = 10$ y $b_1^{\\{1\\}} = -5$)  y NAND ($w_{21}^{\\{1\\}} = -10$, $w_{22}^{\\{1\\}} = -10$ y $b_2^{\\{1\\}} = 15$) respectivamente. La salida de estas 2 neuronas estarían conectadas a una tercera neurona que realiza la operacioón AND ($w_{11}^{\\{2\\}} = 10$, $w_{12}^{\\{2\\}} = 10$ y $b_1^{\\{2\\}} = -15$). En su forma matricial:\n",
        "\n",
        "$$\n",
        "\\mathbf{W}^{\\{1\\}} = \\left[\\begin{matrix} \n",
        "10 & -10\\\\\n",
        "10 & -10\n",
        "\\end{matrix}\\right] \n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{b}^{\\{1\\}} = \\left[\\begin{matrix} \n",
        "-5 \\\\\n",
        "15\n",
        "\\end{matrix}\\right] \n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{W}^{\\{2\\}} = \\left[\\begin{matrix} \n",
        "10\\\\\n",
        "10\n",
        "\\end{matrix}\\right] \n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{b}^{\\{2\\}} = \\left[\\begin{matrix} \n",
        "-15\\\\\n",
        "\\end{matrix}\\right] \n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OtTynPBsPax",
        "outputId": "ec05ead3-8dd8-4510-8950-ba1b7c2d877c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "y_xor = np.array([0., 1., 1., 0.])\n",
        "W1 = np.array([[10, -10], [10, -10]])\n",
        "b1 = np.array([-5, 15])\n",
        "\n",
        "W2 = np.array([[10], [10]])\n",
        "b2 = np.array([-15])\n",
        "\n",
        "print('W_1 = [{0}{1}], b_1 = {2}'.format(W1[0, :], W1[1, :], b1))\n",
        "print('W_2 = [{0}{1}], b_2 = {2}'.format(W2[0], W2[1], b2))\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\ty\\ty_hat')\n",
        "print('-----------------------------')\n",
        "for i in range(X.shape[0]):\n",
        "  y_hat = multicapa(X[i], W1, b1, W2, b2)\n",
        "  print('{0}\\t{1}\\t{2}\\t{3}'.format(X[i, 0], X[i, 1], y_xor[i], y_hat[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W_1 = [[ 10 -10][ 10 -10]], b_1 = [-5 15]\n",
            "W_2 = [[10][10]], b_2 = [-15]\n",
            "-----------------------------\n",
            "x_1 \tx_2 \ty\ty_hat\n",
            "-----------------------------\n",
            "0.0\t0.0\t0.0\t0.0\n",
            "0.0\t1.0\t1.0\t1.0\n",
            "1.0\t0.0\t1.0\t1.0\n",
            "1.0\t1.0\t0.0\t0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}