{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty7Z3RRm2v0P"
   },
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ryOL1_XG4ABQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Descarga dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QSy8yE88JUT",
    "outputId": "41cbb686-75f4-4640-cf45-7fd5487771ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2576k  100 2576k    0     0  9302k      0 --:--:-- --:--:-- --:--:-- 9302k\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4KjIrLz8Ryf",
    "outputId": "f4ace4f8-ec84-4556-cba5-87748562de3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace spa-eng/_about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "gpt_translation.ipynb\t    seq2seq_translation_tutorial.ipynb\tspa-eng.zip\n",
      "gpt_translation_test.ipynb  spa-eng\n"
     ]
    }
   ],
   "source": [
    "!unzip -q spa-eng.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fWwcssJ4ABW"
   },
   "source": [
    "## 1.- Procesa dataset\n",
    "- Procesa las oraciones para traducir de español a inglés y de ingles a español, y agrega token __eos__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQ_74hOZ4ABW",
    "outputId": "e67266de-79af-4cc8-a7e3-a0e181e55a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate English to Spanish: I'll show my album to you. Te enseñaré mi álbum. <eos>\n",
      "translate English to Spanish: Why did the accused confess? ¿Por qué confesaron los acusados? <eos>\n",
      "translate Spanish to English: Esos son los riesgos. Those are the risks. <eos>\n",
      "translate Spanish to English: Tom salía con Mary cuando ambos eran adolescentes. Tom dated Mary when they were both teenagers. <eos>\n",
      "translate Spanish to English: La tienda está cerrada hoy. The shop is closed today. <eos>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "237928"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file = './spa-eng/spa.txt'\n",
    "\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "    \n",
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    text_pairs.append('translate English to Spanish: ' + eng + ' ' + spa + ' <eos>')\n",
    "    text_pairs.append('translate Spanish to English: ' + spa + ' ' + eng + ' <eos>')\n",
    "\n",
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))\n",
    "\n",
    "len(text_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conjuntos de entrenamiento, prueba y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_LIiVqBco41j"
   },
   "outputs": [],
   "source": [
    "random.Random(434).shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIz-c8Rao1fV",
    "outputId": "d05acafd-41e4-4d59-df53-e2df9b96bbfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237928 total pairs\n",
      "166550 training pairs\n",
      "35689 validation pairs\n",
      "35689 test pairs\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "iB8kP0jRSiG4",
    "outputId": "f1ebfa12-fccf-4a18-81f1-5bd4326c9fdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate English to Spanish: The pay is terrible. El pago es terrible. <eos>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWW4RxWhpAZf"
   },
   "source": [
    "- Crea vocabulario y define tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7-V2bNQqpLTE"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0viwLWSUpGrk"
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qpTS_ktRo_xd"
   },
   "outputs": [],
   "source": [
    "def build_vocab(text, tokenizer):\n",
    "    counter = Counter()\n",
    "    for string_ in text:\n",
    "        counter.update(tokenizer(string_))\n",
    "    return vocab(counter, specials=['<unk>', '<pad>', '<eos>'])\n",
    "\n",
    "\n",
    "vocab = build_vocab(train_pairs, tokenizer)\n",
    "vocab.set_default_index(37546) # evita error <ukn>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RlV0maHzt88t",
    "outputId": "fd691500-97f3-483c-ee8b-f3a9dd97fe61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37535"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WX9CyCiZq8gT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166512"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 64\n",
    "\n",
    "def data_process(text):\n",
    "    data = []\n",
    "    for raw_txt in text:\n",
    "        tensor_ = torch.tensor([vocab[token] for token in tokenizer(raw_txt)],\n",
    "                                dtype=torch.long)\n",
    "        if tensor_.shape[0] < maxlen:\n",
    "            x = tensor_[:-1]\n",
    "            y = tensor_[1:]\n",
    "            data.append((x, y))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = data_process(train_pairs)\n",
    "val_data = data_process(val_pairs)\n",
    "test_data = data_process(test_pairs)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN_d_pESTAkZ"
   },
   "source": [
    "## 2.- Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cW6E9jkJTe1o"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "PAD_IDX = vocab['<pad>']\n",
    "EOS_IDX = vocab['<eos>']\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    x, y = [], []\n",
    "    for (x_item, y_item) in data_batch:\n",
    "        x.append(torch.cat([x_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        y.append(torch.cat([y_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "\n",
    "    x = pad_sequence(x, batch_first=True, padding_value=PAD_IDX)\n",
    "    y = pad_sequence(y, batch_first=True, padding_value=PAD_IDX)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
    "                          shuffle=True, collate_fn=generate_batch, \n",
    "                          num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size,\n",
    "                        shuffle=True, collate_fn=generate_batch,\n",
    "                        num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size,\n",
    "                         shuffle=True, collate_fn=generate_batch,\n",
    "                         num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_N6IRYsT4ry",
    "outputId": "a1d1522d-c8db-493b-fc77-e3549aeeb40a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 ms ± 2.86 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "train_batch, target_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aauQX2tFXD8K"
   },
   "outputs": [],
   "source": [
    "train_batch, target_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROw_9EjsT2Et",
    "outputId": "37555466-8faa-49bf-898e-e7374d2c66db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 47]), torch.Size([128, 47]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.shape, target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    3,     6,     5,     4,  1898,    80,  1608,   295, 11389,  1840,\n",
       "        35862,    11,    36,   278,   183,  1053,   108,    38, 35863,   245,\n",
       "        35864,    11,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ7f4DHJreIj"
   },
   "source": [
    "## 3.- Modelo\n",
    "- Definir auto atención producto punto con máscara:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{MultiHead}(Q, K, V) = \\text{Concat}(\\mbox{head}_1,\\mbox{head}_2,\\ldots,\\mbox{head}_h)W^O,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) = \\text{softmax}\\left[\\frac{QW_i^Q(KW_i^K)^T}{\\sqrt{d_k}}\\right]VW_i^V,\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dT6uffV0rdh_",
    "outputId": "1f783aa2-47b4-4c1f-b243-e512ee31a3ed",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4971,  0.1038,  0.5954,  ..., -0.1803,  0.1757,  0.2792],\n",
       "         [-0.4971,  0.1038,  0.5954,  ..., -0.1803,  0.1757,  0.2792],\n",
       "         [-0.4971,  0.1038,  0.5954,  ..., -0.1803,  0.1757,  0.2792],\n",
       "         ...,\n",
       "         [-0.4971,  0.1038,  0.5954,  ..., -0.1803,  0.1757,  0.2792],\n",
       "         [-0.4971,  0.1038,  0.5954,  ..., -0.1803,  0.1757,  0.2792],\n",
       "         [-0.4971,  0.1038,  0.5954,  ..., -0.1803,  0.1757,  0.2792]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, maxlen, n_heads=4, bias=True):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = (dim // n_heads) ** -0.5\n",
    "        self.qw = nn.Linear(dim, dim, bias = bias)\n",
    "        self.kw = nn.Linear(dim, dim, bias = bias)\n",
    "        self.vw = nn.Linear(dim, dim, bias = bias)\n",
    "\n",
    "        self.ow = nn.Linear(dim, dim, bias = bias)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(maxlen, maxlen)).view(1, 1, maxlen, maxlen))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        q = self.qw(x)\n",
    "        k = self.qw(x)\n",
    "        v = self.qw(x)\n",
    "\n",
    "        B, L, D = q.shape\n",
    "        q = torch.reshape(q, [B, L, self.n_heads, -1])\n",
    "        q = torch.permute(q, [0, 2, 1, 3])\n",
    "        k = torch.reshape(k, [B, L, self.n_heads, -1])\n",
    "        k = torch.permute(k, [0, 2, 3, 1])\n",
    "        v = torch.reshape(v, [B, L, self.n_heads, -1])\n",
    "        v = torch.permute(v, [0, 2, 1, 3])\n",
    "\n",
    "        qk = torch.matmul(q, k) * self.scale\n",
    "        qk = qk.masked_fill(self.bias[:,:,:L,:L] == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(qk, dim=-1)\n",
    "\n",
    "        v_attn = torch.matmul(attn, v)\n",
    "        v_attn = torch.permute(v_attn, [0, 2, 1, 3])\n",
    "        v_attn = torch.reshape(v_attn, [B, L, D])\n",
    "\n",
    "        x = self.ow(v_attn)\n",
    "        return x\n",
    "\n",
    "\n",
    "test_layer = Attention(32, maxlen, n_heads=1)\n",
    "test_layer(torch.ones([1, maxlen, 32]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Definir Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqDwbCJCAO0A",
    "outputId": "195c2c34-353d-40e9-e913-e2875c8333e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 32])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, maxlen, heads=4, mlp_dim=512, rate=0.0):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, maxlen)\n",
    "        self.ln_2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(rate),\n",
    "            nn.Linear(mlp_dim, dim),\n",
    "            nn.Dropout(rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.ln_1(x)) + x\n",
    "        return self.mlp(self.ln_2(x)) + x\n",
    "\n",
    "test_layer = Transformer(32, maxlen)\n",
    "test_layer(torch.ones([1, maxlen, 32])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WskSwSw_A3g5",
    "outputId": "7a4b546a-b3c6-4451-be83-d5b06453b414"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 47])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pR1QIctMBNj5",
    "outputId": "b906948d-4c9a-41b1-ec20-658adbd41af6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    3,     6,     5,     4,  1898,    80,  1608,   295, 11389,  1840,\n",
       "         35862,    11,    36,   278,   183,  1053,   108,    38, 35863,   245,\n",
       "         35864,    11,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1],\n",
       "        [    3,     6,     5,     4,    77,    78,  1025,   897,   920,    11,\n",
       "            15,    83,    16,    84,  1027,    92,   917,    11,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Definir GPT y agregar embedding de posición:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNOuhWgj_o8z",
    "outputId": "6586f440-b62a-4986-f356-6cac17fbdffe",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 47, 37535]), torch.Size([128, 47]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, dim, vocab_size, maxlen, depth=3, \n",
    "                 mlp_dim=512, rate=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, maxlen, dim))\n",
    "\n",
    "        self.transformer = nn.Sequential()\n",
    "        for _ in range(depth):\n",
    "            self.transformer.append(Transformer(dim, maxlen))\n",
    "\n",
    "        self.head = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.shape\n",
    "        x = self.embedding(x)\n",
    "        x += self.pos_embedding[:, :L]\n",
    "        x = self.transformer(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "model_dim = 128\n",
    "depth = 3\n",
    "mlp_dim = 128\n",
    "\n",
    "gpt = GPT(dim=model_dim, vocab_size=vocab_size, \n",
    "          maxlen=maxlen, depth=depth, mlp_dim=mlp_dim)\n",
    "output = gpt(train_batch)\n",
    "output.shape, target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsJbLyZ6b_57"
   },
   "source": [
    "## 4.- Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvsuJHMBlm9y",
    "outputId": "ff57e59e-6632-40b4-d81c-085cbcc20a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embedding): Embedding(37535, 128)\n",
       "  (transformer): Sequential(\n",
       "    (0): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=128, out_features=37535, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_IDX = vocab.get_stoi()['<pad>']\n",
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wl9MQCp9TLCh"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(gpt.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "R5bmAldOcJn0"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    start = time.time()\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        targets = targets.view(-1)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'\\nTime for epoch {epoch} is {time.time()-start} sec')\n",
    "    print(f'Train loss: {running_loss / len(train_loader):4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Traducción autorregresiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "translate spanish to english me gustan los perros stopped bills jerga sustituyera stepdad vengo terremotos mías twist center transmit enfrente amid invierte construction ¿tomas combustibles permitas puedes politely custom-made cooperó rival ride enseñaros ensayo apañan critical relaja despegaron ai trench presentame compromise serrando tarro unusual ¡incendio buscabas trabajólicos anestésico raccoons momentáneamente ocaso checoslovaquia ¿lees apoya taimado\n",
      "\n",
      "translate spanish to english me gustan los gatos withdraw indispensable sixtina neptune prowlers forty field sostienes war center transmit enfrente amid invierte construction ¿tomas combustibles permitas puedes politely custom-made cooperó rival ride enseñaros ensayo hub extract excepto stink merecía forgive debés introduce tenerlas contratos taza anaconda consígueme enseñará atender riéndome podíamos alphabet continue naciera spain fascinating\n",
      "\n",
      "translate english to spanish tom is a big dog originó barrita afectarán asistiré cuarta construyeran carné tía apologizing deletrearon adopte reflejo apostarlo medios reemplazar encantaste telefonearle prohibits politely custom-made cooperó rival ride enseñaros ensayo apañan critical relaja despegaron ai trench presentame compromise serrando tarro unusual ¡incendio buscabas trabajólicos anestésico abhorrent muelas saving británico quedaras happy-go-lucky\n"
     ]
    }
   ],
   "source": [
    "def translate(model, sentence, device, maxlen):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([vocab[token] for token in tokenizer(sentence)],\n",
    "                                dtype=torch.long)\n",
    "    idx = idx.reshape([1, -1])\n",
    "    maxlen = maxlen - idx.shape[-1]\n",
    "    \n",
    "    for _ in range(maxlen):\n",
    "        idx = idx.to(device)\n",
    "        logits = gpt(idx)[:, -1, :]      \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "    txt = \" \".join(\n",
    "                [vocab.get_itos()[idx[0, _]] for _ in range(maxlen)]\n",
    "            )\n",
    "    return txt.replace(\"<eos>\", \"\")\n",
    "        \n",
    "sentences = ['translate spanish to english me gustan los perros',\n",
    "             'translate spanish to english me gustan los gatos',\n",
    "             'translate english to spanish tom is a big dog']\n",
    "\n",
    "for sent in sentences:\n",
    "    trans = translate(gpt, sent, device, maxlen)\n",
    "    print(f\"\\n{trans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gtR1RoSBkARg",
    "outputId": "5e15136d-9e74-4734-d9d0-307ddbcd59d0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for epoch 0 is 29.74670433998108 sec\n",
      "Train loss: 3.588693\n",
      "translate spanish to english me gustan los perros . i ' ll be a lot of you .                                      \n",
      "translate spanish to english me gustan los gatos . i ' ll be a lot of you .                                      \n",
      "translate english to spanish tom is a big dog . tom es un buen trabajo .                                       \n",
      "\n",
      "Time for epoch 1 is 30.006539583206177 sec\n",
      "Train loss: 2.822797\n",
      "translate spanish to english me gustan los perros nadaran a la guerra . i like the children to go to the party .                                 \n",
      "translate spanish to english me gustan los gatos . i like the rules .                                          \n",
      "translate english to spanish tom is a big dog . tom es un perro .                                        \n",
      "\n",
      "Time for epoch 2 is 30.240260124206543 sec\n",
      "Train loss: 2.518456\n",
      "translate spanish to english me gustan los perros . i like noodles .                                           \n",
      "translate spanish to english me gustan los gatos . i like cats are neighbors .                                         \n",
      "translate english to spanish tom is a big dog . tom es un perro marrón a perro .                                     \n",
      "\n",
      "Time for epoch 3 is 30.098933458328247 sec\n",
      "Train loss: 2.290070\n",
      "translate spanish to english me gustan los perros . i like dogs .                                           \n",
      "translate spanish to english me gustan los gatos . i like cats are mine .                                         \n",
      "translate english to spanish tom is a big dog . tom es un perro marrón .                                       \n",
      "\n",
      "Time for epoch 4 is 30.107249975204468 sec\n",
      "Train loss: 2.117958\n",
      "translate spanish to english me gustan los perros . i like dogs .                                           \n",
      "translate spanish to english me gustan los gatos . i like cats .                                           \n",
      "translate english to spanish tom is a big dog . tom es un perro grande .                                       \n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(gpt, device, train_loader, optimizer, epoch)\n",
    "    \n",
    "    # Translate test sentences\n",
    "    for sent in sentences:\n",
    "        trans = translate(gpt, sent, device, maxlen)\n",
    "        print(trans)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
