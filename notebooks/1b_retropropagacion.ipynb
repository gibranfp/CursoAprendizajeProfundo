{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1b_retropropagacion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gibranfp/CursoAprendizajeProfundo/blob/2022-1/notebooks/1b_retropropagacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V83__FrBij1f"
      },
      "source": [
        "# Retropropagación\n",
        "\n",
        "En este *notebook* programaremos con NumPy una red neuronal densa y la entrenaremos para aproximar la operación XOR usando del gradiente descedente con el algoritmo de retropropagación. Recordemos que la operación XOR ($\\otimes$) está de la siguiente manera:\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ------------- |:-------------:| -----:|\n",
        "|0 |0 |0|\n",
        "|0 |1 |1|\n",
        "|1 |0 |1|\n",
        "|1 |1 |0|\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSlnjW4Oi-FP"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iAUmKI5jNuX"
      },
      "source": [
        "Nuestra red neuronal densa está compuesta por una capa de 2 entradas ($x_1$ y $x_2$), una capa oculta con 10 neuronas con función de activación sigmoide y una capa de salida con una sola neurona con función de activación sigmoide. Esta función de activación se define como:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYhT3i68jf6x"
      },
      "source": [
        "def sigmoide(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx6SyrPhWBrw"
      },
      "source": [
        "La función sigmoide tiene una derivada que está expresada en términos de la misma función, esto es, \n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\sigma (z)}{\\partial z} = \\sigma(z) (1 - \\sigma(z))\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJxvxKeAjn24"
      },
      "source": [
        "def derivada_sigmoide(x):\n",
        "    return np.multiply(sigmoide(x), (1.0 - sigmoide(x)))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WxI8FfLXKHv"
      },
      "source": [
        "Podemos ver la operación XOR como una tarea de clasificación binaria a partir de 2 entradas. Por lo tanto, usaremos la función de pérdida de entropía cruzada binaria:\n",
        "\n",
        "$$\n",
        "ECB(\\mathbf{y}, \\mathbf{\\hat{y}})  = -\\sum_{i=1}^N \\left[ y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)}) \\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDjlmpAQjR3X"
      },
      "source": [
        "def entropia_cruzada_binaria(y, p):\n",
        "    p[p == 0] = np.nextafter(0., 1.)\n",
        "    p[p == 1] = np.nextafter(1., 0.)\n",
        "    return -(np.log(p[y == 1]).sum() + np.log(1 - p[y == 0]).sum())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8nMdK-RYWMS"
      },
      "source": [
        "Asimismo, calcularemos la exactitud para medir el rendimiento del modelo aprendido por la red neuronal densa:\n",
        "\n",
        "$$\n",
        "exactitud = \\frac{correctos}{total}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wxvZq10jIM3"
      },
      "source": [
        "def exactitud(y, y_predicha):\n",
        "    return (y == y_predicha).mean() * 100"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p02hAdUFZNLL"
      },
      "source": [
        "Ahora, definimos la función que propaga hacia adelante una entrada $\\mathbf{x}^{i}$. Como la red está compuesta de 2 capas densas (1 oculta y 1 de salida), tenemos 2 matrices de pesos con sus correspondientes vectores de sesgos $\\{\\mathbf{W}^{\\{1\\}}, \\mathbf{b}^{\\{1\\}}\\}$ y $\\{\\mathbf{W}^{\\{2\\}}, \\mathbf{b}^{\\{2\\}}\\}$ de la capa oculta y la capa de salida respectivamente. Así, podemos llevar a cabo la propagación hacia adelante en esta red de la siguiente manera:\n",
        "\n",
        "$$\n",
        "\t\\begin{split}\n",
        "\t\t\t\t\\mathbf{a}^{\\{1\\}} & =  \\mathbf{x}^{(i)} \\\\\n",
        "\t\t\t\t\\mathbf{z}^{\\{2\\}} & =  \\mathbf{W}^{\\{1\\}} \\cdot \\mathbf{a}^{\\{1\\}} + \\mathbf{b}^{\\{1\\}}\\\\\n",
        "\t\t\t\t\\mathbf{a}^{\\{2\\}} & =  \\sigma(\\mathbf{z}^{\\{2\\}}) \\\\\n",
        "\t\t\t\t\\mathbf{z}^{\\{3\\}} & =  \\mathbf{W}^{\\{2\\}} \\cdot \\mathbf{a}^{\\{2\\}}  + \\mathbf{b}^{\\{2\\}}\\\\\n",
        "\t\t\t\t\\mathbf{a}^{\\{3\\}} & =  \\sigma(\\mathbf{z}^{\\{3\\}})\\\\\n",
        "\t\t\t\t\\hat{y}^{(i)} & =  \\mathbf{a}^{\\{3\\}}\n",
        "\t\t\t\\end{split}\n",
        "      $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAsEk-zajvpX"
      },
      "source": [
        "def hacia_adelante(x, W1, b1, W2, b2):\n",
        "    z2 = np.dot(W1.T, x[:, np.newaxis]) + b1\n",
        "    a2 = sigmoide(z2)\n",
        "    z3 = np.dot(W2.T, a2) + b2\n",
        "    y_hat = sigmoide(z3)\n",
        "    return z2, a2, z3, y_hat"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiOT6jqXjzwQ"
      },
      "source": [
        "Finalmente, definimos la función para entrenar nuestra red neuronal usando gradiente descendente. Para calcular el gradiente de la función de pérdida respecto a los pesos y sesgos en cada capa empleamos el algoritmo de retropropagación.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P7i6eLgkJdg"
      },
      "source": [
        "def retropropagacion(X, y, alpha = 0.01, n_epocas = 100, n_ocultas = 10):\n",
        "    n_ejemplos = X.shape[0]\n",
        "    n_entradas = X.shape[1]\n",
        "        \n",
        "    # Inicialización de las matrices de pesos W y V\n",
        "    W1 = np.sqrt(1.0 / n_entradas) * np.random.randn(n_entradas, n_ocultas)\n",
        "    b1 = np.zeros((n_ocultas, 1))\n",
        "    \n",
        "    W2 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, 1)\n",
        "    b2 = np.zeros((1, 1))\n",
        "    \n",
        "    perdidas = np.zeros((n_epocas))\n",
        "    exactitudes = np.zeros((n_epocas))\n",
        "    y_predicha = np.zeros((y.shape))\n",
        "    for i in range(n_epocas):\n",
        "        for j in range(n_ejemplos):\n",
        "            z2, a2, z3, y_hat = hacia_adelante(X[j], W1, b1, W2, b2)\n",
        "\n",
        "            # cálculo de gradientes para W2 y b2 por retropropagación\n",
        "            dz3 = y_hat - y[j]\n",
        "            dW2 = np.outer(a2, dz3)\n",
        "            db2 = dz3\n",
        "\n",
        "            # cálculo de gradientes para W1 y b1 por retropropagación\n",
        "            dz2 = np.dot(W2, dz3) * derivada_sigmoide(z2)\n",
        "            dW1 = np.outer(X[j], dz2)\n",
        "            db1 = dz2\n",
        "            \n",
        "            ####################################\n",
        "            # IMPORTANTE \n",
        "            # la actualización de los parámetros\n",
        "            # debe hacerse de forma simultánea\n",
        "            W2 = W2 - alpha * dW2\n",
        "            b2 = b2 - alpha * db2\n",
        "            W1 = W1 - alpha * dW1\n",
        "            b1 = b1 - alpha * db1\n",
        "\n",
        "            y_predicha[j] = y_hat\n",
        "            \n",
        "        # calcula la pérdida en la época\n",
        "        perdidas[i] = entropia_cruzada_binaria(y, y_predicha)\n",
        "        exactitudes[i] = exactitud(y, np.round(y_predicha))\n",
        "        print('Epoch {0}: Pérdida = {1} Exactitud = {2}'.format(i, \n",
        "                                                              perdidas[i], \n",
        "                                                              exactitudes[i]))\n",
        "\n",
        "    return W1, W2, perdidas, exactitudes"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nau0HWsrkRxg"
      },
      "source": [
        "Para probar nuestra red, generamos los ejemplos correspondientes a la operación XOR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8txXZ34GkUAF"
      },
      "source": [
        "# ejemplo (XOR)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0, 1, 1, 0]]).T"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLT8avfhkYH7"
      },
      "source": [
        "Finalmente, entrenamos nuestra red con estos ejemplos por 200 épocas usando una tasa de aprendizaje $\\alpha = 1.0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijKxVwZ3kbyR",
        "outputId": "230f6290-0191-4748-b71a-6dae1c886e81"
      },
      "source": [
        "np.random.seed(0)\n",
        "W1, W2, perdidas, exactitudes = retropropagacion(X, \n",
        "                                                 y, \n",
        "                                                 alpha = 1.0, \n",
        "                                                 n_epocas = 200,\n",
        "                                                 n_ocultas = 5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: Pérdida = 4.759342701710854 Exactitud = 25.0\n",
            "Epoch 1: Pérdida = 4.590954054596982 Exactitud = 50.0\n",
            "Epoch 2: Pérdida = 4.4120467310968845 Exactitud = 25.0\n",
            "Epoch 3: Pérdida = 4.2411690549343275 Exactitud = 25.0\n",
            "Epoch 4: Pérdida = 4.101747602832166 Exactitud = 25.0\n",
            "Epoch 5: Pérdida = 3.99322071947935 Exactitud = 25.0\n",
            "Epoch 6: Pérdida = 3.908739595255105 Exactitud = 25.0\n",
            "Epoch 7: Pérdida = 3.8410611618056363 Exactitud = 25.0\n",
            "Epoch 8: Pérdida = 3.785052250686191 Exactitud = 25.0\n",
            "Epoch 9: Pérdida = 3.7375572526301 Exactitud = 25.0\n",
            "Epoch 10: Pérdida = 3.6966571576643146 Exactitud = 25.0\n",
            "Epoch 11: Pérdida = 3.6611124774461197 Exactitud = 25.0\n",
            "Epoch 12: Pérdida = 3.630050227881158 Exactitud = 25.0\n",
            "Epoch 13: Pérdida = 3.6028044528771344 Exactitud = 25.0\n",
            "Epoch 14: Pérdida = 3.5788368437547042 Exactitud = 25.0\n",
            "Epoch 15: Pérdida = 3.5576963490499685 Exactitud = 25.0\n",
            "Epoch 16: Pérdida = 3.538997365528072 Exactitud = 50.0\n",
            "Epoch 17: Pérdida = 3.5224068408488183 Exactitud = 50.0\n",
            "Epoch 18: Pérdida = 3.5076357656710684 Exactitud = 50.0\n",
            "Epoch 19: Pérdida = 3.494432927461653 Exactitud = 50.0\n",
            "Epoch 20: Pérdida = 3.4825799111933744 Exactitud = 50.0\n",
            "Epoch 21: Pérdida = 3.4718868579962856 Exactitud = 50.0\n",
            "Epoch 22: Pérdida = 3.462188746014879 Exactitud = 50.0\n",
            "Epoch 23: Pérdida = 3.453342079899679 Exactitud = 50.0\n",
            "Epoch 24: Pérdida = 3.4452219324230056 Exactitud = 50.0\n",
            "Epoch 25: Pérdida = 3.4377193060537574 Exactitud = 50.0\n",
            "Epoch 26: Pérdida = 3.4307387907480438 Exactitud = 50.0\n",
            "Epoch 27: Pérdida = 3.424196495616801 Exactitud = 50.0\n",
            "Epoch 28: Pérdida = 3.418018231138607 Exactitud = 50.0\n",
            "Epoch 29: Pérdida = 3.4121379174131703 Exactitud = 50.0\n",
            "Epoch 30: Pérdida = 3.4064961934921243 Exactitud = 50.0\n",
            "Epoch 31: Pérdida = 3.401039203288043 Exactitud = 50.0\n",
            "Epoch 32: Pérdida = 3.395717534848915 Exactitud = 50.0\n",
            "Epoch 33: Pérdida = 3.3904852916877264 Exactitud = 50.0\n",
            "Epoch 34: Pérdida = 3.385299277168504 Exactitud = 50.0\n",
            "Epoch 35: Pérdida = 3.3801182755070833 Exactitud = 50.0\n",
            "Epoch 36: Pérdida = 3.3749024156381022 Exactitud = 50.0\n",
            "Epoch 37: Pérdida = 3.3696126069712293 Exactitud = 50.0\n",
            "Epoch 38: Pérdida = 3.364210038892513 Exactitud = 50.0\n",
            "Epoch 39: Pérdida = 3.3586557387721476 Exactitud = 50.0\n",
            "Epoch 40: Pérdida = 3.352910186243977 Exactitud = 50.0\n",
            "Epoch 41: Pérdida = 3.3469329846517315 Exactitud = 50.0\n",
            "Epoch 42: Pérdida = 3.3406825938253326 Exactitud = 50.0\n",
            "Epoch 43: Pérdida = 3.334116131739006 Exactitud = 50.0\n",
            "Epoch 44: Pérdida = 3.3271892560422502 Exactitud = 50.0\n",
            "Epoch 45: Pérdida = 3.319856139803386 Exactitud = 50.0\n",
            "Epoch 46: Pérdida = 3.3120695588275604 Exactitud = 50.0\n",
            "Epoch 47: Pérdida = 3.3037811102586367 Exactitud = 50.0\n",
            "Epoch 48: Pérdida = 3.294941583376886 Exactitud = 50.0\n",
            "Epoch 49: Pérdida = 3.2855015029779553 Exactitud = 50.0\n",
            "Epoch 50: Pérdida = 3.2754118628004703 Exactitud = 50.0\n",
            "Epoch 51: Pérdida = 3.2646250604911167 Exactitud = 50.0\n",
            "Epoch 52: Pérdida = 3.253096035997565 Exactitud = 50.0\n",
            "Epoch 53: Pérdida = 3.2407836017683347 Exactitud = 50.0\n",
            "Epoch 54: Pérdida = 3.227651935873598 Exactitud = 50.0\n",
            "Epoch 55: Pérdida = 3.21367218893455 Exactitud = 50.0\n",
            "Epoch 56: Pérdida = 3.1988241341188957 Exactitud = 50.0\n",
            "Epoch 57: Pérdida = 3.1830977687720465 Exactitud = 50.0\n",
            "Epoch 58: Pérdida = 3.1664947595011492 Exactitud = 50.0\n",
            "Epoch 59: Pérdida = 3.1490296130173707 Exactitud = 50.0\n",
            "Epoch 60: Pérdida = 3.130730455867167 Exactitud = 50.0\n",
            "Epoch 61: Pérdida = 3.111639319593274 Exactitud = 50.0\n",
            "Epoch 62: Pérdida = 3.091811854622275 Exactitud = 50.0\n",
            "Epoch 63: Pérdida = 3.071316435067618 Exactitud = 50.0\n",
            "Epoch 64: Pérdida = 3.050232664303763 Exactitud = 50.0\n",
            "Epoch 65: Pérdida = 3.0286493423347247 Exactitud = 50.0\n",
            "Epoch 66: Pérdida = 3.006662004152053 Exactitud = 50.0\n",
            "Epoch 67: Pérdida = 2.984370176805265 Exactitud = 50.0\n",
            "Epoch 68: Pérdida = 2.961874526249 Exactitud = 50.0\n",
            "Epoch 69: Pérdida = 2.939274069897512 Exactitud = 50.0\n",
            "Epoch 70: Pérdida = 2.9166636169001388 Exactitud = 50.0\n",
            "Epoch 71: Pérdida = 2.894131568188712 Exactitud = 50.0\n",
            "Epoch 72: Pérdida = 2.8717581674868775 Exactitud = 50.0\n",
            "Epoch 73: Pérdida = 2.849614249108429 Exactitud = 50.0\n",
            "Epoch 74: Pérdida = 2.827760484808438 Exactitud = 50.0\n",
            "Epoch 75: Pérdida = 2.8062470952408454 Exactitud = 50.0\n",
            "Epoch 76: Pérdida = 2.785113964832152 Exactitud = 50.0\n",
            "Epoch 77: Pérdida = 2.764391083129122 Exactitud = 50.0\n",
            "Epoch 78: Pérdida = 2.7440992301734695 Exactitud = 50.0\n",
            "Epoch 79: Pérdida = 2.7242508262629466 Exactitud = 50.0\n",
            "Epoch 80: Pérdida = 2.7048508750938476 Exactitud = 50.0\n",
            "Epoch 81: Pérdida = 2.6858979412583617 Exactitud = 50.0\n",
            "Epoch 82: Pérdida = 2.66738511626703 Exactitud = 50.0\n",
            "Epoch 83: Pérdida = 2.6493009400957477 Exactitud = 50.0\n",
            "Epoch 84: Pérdida = 2.6316302566964955 Exactitud = 50.0\n",
            "Epoch 85: Pérdida = 2.6143549914318767 Exactitud = 50.0\n",
            "Epoch 86: Pérdida = 2.5974548458410993 Exactitud = 50.0\n",
            "Epoch 87: Pérdida = 2.5809079106153425 Exactitud = 50.0\n",
            "Epoch 88: Pérdida = 2.564691201391569 Exactitud = 50.0\n",
            "Epoch 89: Pérdida = 2.5487811242630225 Exactitud = 50.0\n",
            "Epoch 90: Pérdida = 2.5331538790526276 Exactitud = 50.0\n",
            "Epoch 91: Pérdida = 2.5177858086733944 Exactitud = 50.0\n",
            "Epoch 92: Pérdida = 2.5026537025340665 Exactitud = 50.0\n",
            "Epoch 93: Pérdida = 2.487735061116659 Exactitud = 50.0\n",
            "Epoch 94: Pérdida = 2.473008327688671 Exactitud = 50.0\n",
            "Epoch 95: Pérdida = 2.4584530917139595 Exactitud = 50.0\n",
            "Epoch 96: Pérdida = 2.4440502669625217 Exactitud = 50.0\n",
            "Epoch 97: Pérdida = 2.4297822456439704 Exactitud = 50.0\n",
            "Epoch 98: Pérdida = 2.415633028146681 Exactitud = 50.0\n",
            "Epoch 99: Pérdida = 2.4015883261977207 Exactitud = 50.0\n",
            "Epoch 100: Pérdida = 2.3876356355135737 Exactitud = 50.0\n",
            "Epoch 101: Pérdida = 2.373764272338148 Exactitud = 50.0\n",
            "Epoch 102: Pérdida = 2.3599653667121734 Exactitud = 50.0\n",
            "Epoch 103: Pérdida = 2.3462318039271186 Exactitud = 50.0\n",
            "Epoch 104: Pérdida = 2.3325581044020556 Exactitud = 50.0\n",
            "Epoch 105: Pérdida = 2.3189402311502256 Exactitud = 50.0\n",
            "Epoch 106: Pérdida = 2.30537531296165 Exactitud = 50.0\n",
            "Epoch 107: Pérdida = 2.2918612701914514 Exactitud = 50.0\n",
            "Epoch 108: Pérdida = 2.278396328222259 Exactitud = 50.0\n",
            "Epoch 109: Pérdida = 2.2649784006596683 Exactitud = 50.0\n",
            "Epoch 110: Pérdida = 2.2516043192340196 Exactitud = 50.0\n",
            "Epoch 111: Pérdida = 2.2382688789518466 Exactitud = 50.0\n",
            "Epoch 112: Pérdida = 2.2249636534763795 Exactitud = 50.0\n",
            "Epoch 113: Pérdida = 2.2116755144876024 Exactitud = 50.0\n",
            "Epoch 114: Pérdida = 2.1983847562651286 Exactitud = 50.0\n",
            "Epoch 115: Pérdida = 2.185062677745405 Exactitud = 50.0\n",
            "Epoch 116: Pérdida = 2.1716684013223464 Exactitud = 50.0\n",
            "Epoch 117: Pérdida = 2.158144600066599 Exactitud = 50.0\n",
            "Epoch 118: Pérdida = 2.1444116486247276 Exactitud = 50.0\n",
            "Epoch 119: Pérdida = 2.1303594914016575 Exactitud = 50.0\n",
            "Epoch 120: Pérdida = 2.1158362236270944 Exactitud = 50.0\n",
            "Epoch 121: Pérdida = 2.100632025958033 Exactitud = 50.0\n",
            "Epoch 122: Pérdida = 2.0844568024497723 Exactitud = 50.0\n",
            "Epoch 123: Pérdida = 2.066910048298623 Exactitud = 50.0\n",
            "Epoch 124: Pérdida = 2.0474432074808466 Exactitud = 50.0\n",
            "Epoch 125: Pérdida = 2.0253205650007593 Exactitud = 50.0\n",
            "Epoch 126: Pérdida = 1.9995989778877492 Exactitud = 50.0\n",
            "Epoch 127: Pérdida = 1.9691729518084253 Exactitud = 50.0\n",
            "Epoch 128: Pérdida = 1.9329564169272255 Exactitud = 50.0\n",
            "Epoch 129: Pérdida = 1.8902321601453442 Exactitud = 75.0\n",
            "Epoch 130: Pérdida = 1.8410023907993027 Exactitud = 75.0\n",
            "Epoch 131: Pérdida = 1.7859647360653184 Exactitud = 75.0\n",
            "Epoch 132: Pérdida = 1.7260439634604914 Exactitud = 75.0\n",
            "Epoch 133: Pérdida = 1.662044961592691 Exactitud = 75.0\n",
            "Epoch 134: Pérdida = 1.5947484524024769 Exactitud = 75.0\n",
            "Epoch 135: Pérdida = 1.5250801645580936 Exactitud = 75.0\n",
            "Epoch 136: Pérdida = 1.4540960271226497 Exactitud = 75.0\n",
            "Epoch 137: Pérdida = 1.3828666943066006 Exactitud = 75.0\n",
            "Epoch 138: Pérdida = 1.3123649142729614 Exactitud = 100.0\n",
            "Epoch 139: Pérdida = 1.2433933029233801 Exactitud = 100.0\n",
            "Epoch 140: Pérdida = 1.1765598411540872 Exactitud = 100.0\n",
            "Epoch 141: Pérdida = 1.1122922976312686 Exactitud = 100.0\n",
            "Epoch 142: Pérdida = 1.0508735788423533 Exactitud = 100.0\n",
            "Epoch 143: Pérdida = 0.9924802380354665 Exactitud = 100.0\n",
            "Epoch 144: Pérdida = 0.9372131422938436 Exactitud = 100.0\n",
            "Epoch 145: Pérdida = 0.8851171267157283 Exactitud = 100.0\n",
            "Epoch 146: Pérdida = 0.8361917064685593 Exactitud = 100.0\n",
            "Epoch 147: Pérdida = 0.7903967893997141 Exactitud = 100.0\n",
            "Epoch 148: Pérdida = 0.7476568373571184 Exactitud = 100.0\n",
            "Epoch 149: Pérdida = 0.7078654728360956 Exactitud = 100.0\n",
            "Epoch 150: Pérdida = 0.6708911368500872 Exactitud = 100.0\n",
            "Epoch 151: Pérdida = 0.6365835201585621 Exactitud = 100.0\n",
            "Epoch 152: Pérdida = 0.6047801400974356 Exactitud = 100.0\n",
            "Epoch 153: Pérdida = 0.5753124526617586 Exactitud = 100.0\n",
            "Epoch 154: Pérdida = 0.548011080705683 Exactitud = 100.0\n",
            "Epoch 155: Pérdida = 0.5227099629267824 Exactitud = 100.0\n",
            "Epoch 156: Pérdida = 0.49924941051647515 Exactitud = 100.0\n",
            "Epoch 157: Pérdida = 0.47747817585119684 Exactitud = 100.0\n",
            "Epoch 158: Pérdida = 0.4572546957377361 Exactitud = 100.0\n",
            "Epoch 159: Pérdida = 0.43844768676296064 Exactitud = 100.0\n",
            "Epoch 160: Pérdida = 0.4209362591822101 Exactitud = 100.0\n",
            "Epoch 161: Pérdida = 0.4046096918472758 Exactitud = 100.0\n",
            "Epoch 162: Pérdida = 0.3893669827899989 Exactitud = 100.0\n",
            "Epoch 163: Pérdida = 0.375116263329765 Exactitud = 100.0\n",
            "Epoch 164: Pérdida = 0.3617741403871373 Exactitud = 100.0\n",
            "Epoch 165: Pérdida = 0.34926501283516476 Exactitud = 100.0\n",
            "Epoch 166: Pérdida = 0.3375203930760653 Exactitud = 100.0\n",
            "Epoch 167: Pérdida = 0.326478254050256 Exactitud = 100.0\n",
            "Epoch 168: Pérdida = 0.3160824138932608 Exactitud = 100.0\n",
            "Epoch 169: Pérdida = 0.30628196479915526 Exactitud = 100.0\n",
            "Epoch 170: Pérdida = 0.2970307487575162 Exactitud = 100.0\n",
            "Epoch 171: Pérdida = 0.2882868802399555 Exactitud = 100.0\n",
            "Epoch 172: Pérdida = 0.2800123142591562 Exactitud = 100.0\n",
            "Epoch 173: Pérdida = 0.2721724572324825 Exactitud = 100.0\n",
            "Epoch 174: Pérdida = 0.26473581754951236 Exactitud = 100.0\n",
            "Epoch 175: Pérdida = 0.2576736925196706 Exactitud = 100.0\n",
            "Epoch 176: Pérdida = 0.2509598883561778 Exactitud = 100.0\n",
            "Epoch 177: Pérdida = 0.24457046996078613 Exactitud = 100.0\n",
            "Epoch 178: Pérdida = 0.23848353745818057 Exactitud = 100.0\n",
            "Epoch 179: Pérdida = 0.2326790266540305 Exactitud = 100.0\n",
            "Epoch 180: Pérdida = 0.2271385308327109 Exactitud = 100.0\n",
            "Epoch 181: Pérdida = 0.22184514155427354 Exactitud = 100.0\n",
            "Epoch 182: Pérdida = 0.21678330634572052 Exactitud = 100.0\n",
            "Epoch 183: Pérdida = 0.21193870140327242 Exactitud = 100.0\n",
            "Epoch 184: Pérdida = 0.20729811762723574 Exactitud = 100.0\n",
            "Epoch 185: Pérdida = 0.20284935849795457 Exactitud = 100.0\n",
            "Epoch 186: Pérdida = 0.19858114847015493 Exactitud = 100.0\n",
            "Epoch 187: Pérdida = 0.19448305071443533 Exactitud = 100.0\n",
            "Epoch 188: Pérdida = 0.19054539316970553 Exactitud = 100.0\n",
            "Epoch 189: Pérdida = 0.1867592019903953 Exactitud = 100.0\n",
            "Epoch 190: Pérdida = 0.18311614157852246 Exactitud = 100.0\n",
            "Epoch 191: Pérdida = 0.17960846048464102 Exactitud = 100.0\n",
            "Epoch 192: Pérdida = 0.1762289425445724 Exactitud = 100.0\n",
            "Epoch 193: Pérdida = 0.17297086269185225 Exactitud = 100.0\n",
            "Epoch 194: Pérdida = 0.16982794695016845 Exactitud = 100.0\n",
            "Epoch 195: Pérdida = 0.16679433616670314 Exactitud = 100.0\n",
            "Epoch 196: Pérdida = 0.16386455309715606 Exactitud = 100.0\n",
            "Epoch 197: Pérdida = 0.16103347249714017 Exactitud = 100.0\n",
            "Epoch 198: Pérdida = 0.1582962939132972 Exactitud = 100.0\n",
            "Epoch 199: Pérdida = 0.15564851690157377 Exactitud = 100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8A3KZ5JkDJ3"
      },
      "source": [
        "Graficamos el valor de la pérdida y la exactitud en cada época para ver el comportamiento de nuestra red durante el entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "yglJSF9nkR7k",
        "outputId": "73ee2485-959c-4ef9-eea2-961c7146664f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.arange(perdidas.size), perdidas, label='ECB')\n",
        "plt.plot(np.arange(exactitudes.size), exactitudes, label='Exactitud')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz0OaZtsCCgs"
      },
      "source": [
        "## Inicializando los pesos con zeros\n",
        "Como se mencionó anteriormente, las matrices de pesos $\\mathbf{W^{\\{1\\}}}$ y $\\mathbf{W^{\\{2\\}}}$ se initializan con valores aleatorios pequeños mientras que los vectores de sesgo $\\mathbf{b^{\\{1\\}}}$ y $\\mathbf{b^{\\{1\\}}}$ con zeros. Examinemos qué pasa si inicializamos las matrices de pesos con zeros. Observa los valores de los pesos en cada época."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHaavbWGzqzg"
      },
      "source": [
        "def retropropagacion_zeros(X, y, alpha = 0.1, n_epocas = 100, n_ocultas = 10):\n",
        "    n_ejemplos = X.shape[0]\n",
        "    n_entradas = X.shape[1]\n",
        "    \n",
        "    # Inicializa matrices de pesos W1 y W2 y vectores de sesgos b1 y b2\n",
        "    W1 = np.zeros((n_entradas, n_ocultas))\n",
        "    b1 = np.zeros((n_ocultas, 1)) \n",
        "    W2 = np.zeros((n_ocultas, 1))\n",
        "    b2 = np.zeros((1, 1))\n",
        "    \n",
        "    perdidas = np.zeros((n_epocas))\n",
        "    exactitudes = np.zeros((n_epocas))\n",
        "    y_predicha = np.zeros((y.shape))\n",
        "    for i in range(n_epocas):\n",
        "        for j in range(n_ejemplos):\n",
        "            z2, a2, z3, y_hat = hacia_adelante(X[j], W1, b1, W2, b2)\n",
        "\n",
        "            # cálculo de gradientes para W2 y b2 por retropropagación\n",
        "            dz3 = y_hat - y[j]\n",
        "            dW2 = np.outer(a2, dz3)\n",
        "            db2 = dz3\n",
        "\n",
        "            # cálculo de gradientes para W1 y b1 por retropropagación\n",
        "            dz2 = np.dot(W2, dz3) * derivada_sigmoide(z2)\n",
        "            dW1 = np.outer(X[j], dz2)\n",
        "            db1 = dz2\n",
        "            \n",
        "            ####################################\n",
        "            # IMPORTANTE \n",
        "            # la actualización de los parámetros\n",
        "            # debe hacerse de forma simultánea\n",
        "            W2 = W2 - alpha * dW2\n",
        "            b2 = b2 - alpha * db2\n",
        "            W1 = W1 - alpha * dW1\n",
        "            b1 = b1 - alpha * db1\n",
        "            \n",
        "            y_predicha[j] = y_hat\n",
        "            \n",
        "        # calcula la pérdida en época\n",
        "        perdidas[i] = entropia_cruzada_binaria(y, y_predicha)\n",
        "        exactitudes[i] = exactitud(y, np.round(y_predicha))\n",
        "        print('Epoch {0}: Pérdida = {1} Exactitud = {2}'.format(i, \n",
        "                                                              perdidas[i], \n",
        "                                                              exactitudes[i]))\n",
        "        print('W1 = {0}'.format(W1))\n",
        "        print('W2 = {0}'.format(W2))\n",
        "            \n",
        "    return W1, W2, perdidas, exactitudes"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr1HownICHf9",
        "outputId": "de07e4aa-190a-4633-fca3-f7e53fde5f65"
      },
      "source": [
        "W1, W2, perdidas, exactitudes = retropropagacion_zeros(X, \n",
        "                                                       y, \n",
        "                                                       alpha = 1.0,\n",
        "                                                       n_epocas = 5,\n",
        "                                                       n_ocultas = 10)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: Pérdida = 4.955975401669942 Exactitud = 50.0\n",
            "W1 = [[-0.05285737 -0.05285737 -0.05285737 -0.05285737 -0.05285737 -0.05285737\n",
            "  -0.05285737 -0.05285737 -0.05285737 -0.05285737]\n",
            " [-0.11621509 -0.11621509 -0.11621509 -0.11621509 -0.11621509 -0.11621509\n",
            "  -0.11621509 -0.11621509 -0.11621509 -0.11621509]]\n",
            "W2 = [[-0.13143587]\n",
            " [-0.13143587]\n",
            " [-0.13143587]\n",
            " [-0.13143587]\n",
            " [-0.13143587]\n",
            " [-0.13143587]\n",
            " [-0.13143587]\n",
            " [-0.13143587]\n",
            " [-0.13143587]\n",
            " [-0.13143587]]\n",
            "Epoch 1: Pérdida = 4.451676159347022 Exactitud = 50.0\n",
            "W1 = [[-0.0956547  -0.0956547  -0.0956547  -0.0956547  -0.0956547  -0.0956547\n",
            "  -0.0956547  -0.0956547  -0.0956547  -0.0956547 ]\n",
            " [-0.22424568 -0.22424568 -0.22424568 -0.22424568 -0.22424568 -0.22424568\n",
            "  -0.22424568 -0.22424568 -0.22424568 -0.22424568]]\n",
            "W2 = [[-0.08853636]\n",
            " [-0.08853636]\n",
            " [-0.08853636]\n",
            " [-0.08853636]\n",
            " [-0.08853636]\n",
            " [-0.08853636]\n",
            " [-0.08853636]\n",
            " [-0.08853636]\n",
            " [-0.08853636]\n",
            " [-0.08853636]]\n",
            "Epoch 2: Pérdida = 4.254822716727915 Exactitud = 50.0\n",
            "W1 = [[-0.13163808 -0.13163808 -0.13163808 -0.13163808 -0.13163808 -0.13163808\n",
            "  -0.13163808 -0.13163808 -0.13163808 -0.13163808]\n",
            " [-0.31587778 -0.31587778 -0.31587778 -0.31587778 -0.31587778 -0.31587778\n",
            "  -0.31587778 -0.31587778 -0.31587778 -0.31587778]]\n",
            "W2 = [[-0.05289455]\n",
            " [-0.05289455]\n",
            " [-0.05289455]\n",
            " [-0.05289455]\n",
            " [-0.05289455]\n",
            " [-0.05289455]\n",
            " [-0.05289455]\n",
            " [-0.05289455]\n",
            " [-0.05289455]\n",
            " [-0.05289455]]\n",
            "Epoch 3: Pérdida = 4.123758295004898 Exactitud = 50.0\n",
            "W1 = [[-0.16214926 -0.16214926 -0.16214926 -0.16214926 -0.16214926 -0.16214926\n",
            "  -0.16214926 -0.16214926 -0.16214926 -0.16214926]\n",
            " [-0.39336263 -0.39336263 -0.39336263 -0.39336263 -0.39336263 -0.39336263\n",
            "  -0.39336263 -0.39336263 -0.39336263 -0.39336263]]\n",
            "W2 = [[-0.02643033]\n",
            " [-0.02643033]\n",
            " [-0.02643033]\n",
            " [-0.02643033]\n",
            " [-0.02643033]\n",
            " [-0.02643033]\n",
            " [-0.02643033]\n",
            " [-0.02643033]\n",
            " [-0.02643033]\n",
            " [-0.02643033]]\n",
            "Epoch 4: Pérdida = 4.029568991844007 Exactitud = 50.0\n",
            "W1 = [[-0.18831377 -0.18831377 -0.18831377 -0.18831377 -0.18831377 -0.18831377\n",
            "  -0.18831377 -0.18831377 -0.18831377 -0.18831377]\n",
            " [-0.45933566 -0.45933566 -0.45933566 -0.45933566 -0.45933566 -0.45933566\n",
            "  -0.45933566 -0.45933566 -0.45933566 -0.45933566]]\n",
            "W2 = [[-0.00700516]\n",
            " [-0.00700516]\n",
            " [-0.00700516]\n",
            " [-0.00700516]\n",
            " [-0.00700516]\n",
            " [-0.00700516]\n",
            " [-0.00700516]\n",
            " [-0.00700516]\n",
            " [-0.00700516]\n",
            " [-0.00700516]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}