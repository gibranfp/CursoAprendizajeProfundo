{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gibranfp/CursoAprendizajeProfundo/blob/master/notebooks/4a_ucf11_feats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de características convolucionales de cuadros de video\n",
    "\n",
    "<div style=\"text-align: right\"> Bere et Richardt </div>\n",
    "\n",
    "En esta libreta usaremos un modelo CNN preentreando como extractor de características convolucionales de cuadros del conjunto [UCF11](https://www.crcv.ucf.edu/data/UCF_YouTube_Action.php).\n",
    "\n",
    "<img src=\"https://www.crcv.ucf.edu/data/youtube_snaps.jpg\" />\n",
    "\n",
    "<!-- <img src=\"https://raw.githubusercontent.com/gibranfp/CursoAprendizajeProfundo/master/figs/iou.svg\" width=\"950\" height=\"750\" /> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ny0L2LzogTN-"
   },
   "outputs": [],
   "source": [
    "# sistema de archivos\n",
    "import os\n",
    "# listar archivos por patrón\n",
    "from glob import glob\n",
    "# sistema de archivos\n",
    "from os.path import join\n",
    "# flush!\n",
    "import sys\n",
    "\n",
    "# arreglos multidimensionales\n",
    "import numpy as np\n",
    "# redes neuronales\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as tvm\n",
    "import torchvision.transforms as T\n",
    "# almacenamiento de arreglos multidimensionales\n",
    "import zarr\n",
    "# redes\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import read_video\n",
    "# barras de progreso\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "BATCH_SIZE = 5\n",
    "# numéro de cuadros por video\n",
    "NUM_FRAMES = 10\n",
    "IMG_SIZE = 224\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCF11:\n",
    "\n",
    "    def __init__(self, videos_dir, num_frames, tsfm):\n",
    "        pattern = join(videos_dir, '*', '*', '*.mpg')\n",
    "        self.paths = sorted(glob(pattern))\n",
    "        self.num_frames = num_frames\n",
    "        self.tsfm = tsfm\n",
    "        # removemos videos demasiado cortos\n",
    "        self._filter_out_videos()        \n",
    "        # UCF11 classes\n",
    "        classes = (\n",
    "            'basketball', 'biking', 'diving', 'golf_swing', \n",
    "            'horse_riding', 'soccer_juggling', 'swing', 'tennis_swing',\n",
    "            'trampoline_jumping', 'volleyball_spiking', 'walking'\n",
    "        )\n",
    "        self.cls_idx = {c: i for i, c in enumerate(classes)}\n",
    "    \n",
    "    def _filter_out_videos(self):\n",
    "        \"\"\"Remueve videos con menos de `num_frames` frames.\"\"\"\n",
    "        print(f'Removiendo videos con menos de {self.num_frames} cuadros')\n",
    "        sys.stdout.flush()\n",
    "        too_short = []\n",
    "        for path in tqdm(self.paths):\n",
    "            frames = read_video(path, pts_unit='sec')[0].shape[0]\n",
    "            if frames < self.num_frames:\n",
    "                too_short.append(path)\n",
    "        for path in too_short:\n",
    "            self.paths.remove(path)\n",
    "            \n",
    "    def __getitem__(self, i):\n",
    "        path = self.paths[i]\n",
    "        \n",
    "        # ubtenemos subruta class/group/video.mpg\n",
    "        parts = path.split('/')[4:]\n",
    "        subpath = '-'.join([parts[0], parts[2]])[:-4]\n",
    "        \n",
    "        # leemos el video completo\n",
    "        frames = read_video(path, pts_unit='sec')[0]\n",
    "        # calculamos el salto\n",
    "        step = frames.shape[0] // self.num_frames\n",
    "        # creamos indices saltando\n",
    "        indices = np.arange(0, step * self.num_frames, step)\n",
    "        # calculamos los indices restantes y dividismo entre 2\n",
    "        offset = (frames.shape[0] - indices[-1]) // 2\n",
    "        # recorremos a la derecha para centrar\n",
    "        indices += offset\n",
    "        # seleccionamos los cuadros\n",
    "        frames = frames[indices] \n",
    "        # aplicamos trasformación\n",
    "        frames = self.tsfm(frames)\n",
    "        \n",
    "        # obtenemos etiqueta\n",
    "        y = self.cls_idx[parts[0]]\n",
    "\n",
    "        return subpath, frames, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    \n",
    "# Auxiliar transform to process each frame in the video\n",
    "class SeqTsfm:\n",
    "    \"\"\"Sequential transform applies a given over a sequence.\"\"\"\n",
    "    \n",
    "    def __init__(self, tsfm):\n",
    "        \"\"\"Initializer, `tsfm` is the transform to apply.\"\"\"\n",
    "        self.tsfm = tsfm\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        'Applies `tsfm` over the sequence `x`.'\n",
    "        return [self.tsfm(i) for i in x]\n",
    "\n",
    "tsfm = T.Compose([\n",
    "    # torch.tensor a PIL\n",
    "    SeqTsfm(T.ToPILImage()),\n",
    "    # redimensionamos a 224x224\n",
    "    SeqTsfm(T.Resize(IMG_SIZE)),\n",
    "    # cortamos al centro\n",
    "    SeqTsfm(T.CenterCrop(IMG_SIZE)),\n",
    "    # convertimos a torch.Tensor [3,H,W]\n",
    "    # y escalamos a [0,1]\n",
    "    SeqTsfm(T.ToTensor()),\n",
    "    # estandarizamos con media y desviación estandar\n",
    "    SeqTsfm(T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)),\n",
    "    # apilamos los cuadros\n",
    "    torch.stack,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando ../data/ucf11/UCF11_updated_mpg\n",
      "Removiendo videos con menos de 10 cuadros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [01:48<00:00, 14.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo características en ../data/ucf11/ucf11.zarr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 320/320 [03:22<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = join(DATA_DIR, 'ucf11')\n",
    "zarr_dir = join(data_dir, 'ucf11.zarr')\n",
    "\n",
    "if not os.path.isdir(zarr_dir):\n",
    "    \n",
    "    videos_dir = join(data_dir, 'UCF11_updated_mpg')\n",
    "    print(f'Usando {videos_dir}')\n",
    "\n",
    "    ds = UCF11(videos_dir, NUM_FRAMES, tsfm)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SIZE, num_workers=4)\n",
    "\n",
    "    print(f'Extrayendo características en {zarr_dir}')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = tvm.densenet121(pretrained=True)\n",
    "    model.classifier = nn.Identity()\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    z = zarr.open(zarr_dir, 'w')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for subpaths, frames, ys in tqdm(dl):\n",
    "        \n",
    "            frames = frames.to(device)\n",
    "\n",
    "            # guardamos dimensiones\n",
    "            b, s, h, w, c = frames.shape\n",
    "            # planamos lote y secuencia en una sola dimensión\n",
    "            frames = frames.reshape(-1, h, w, c)\n",
    "            # computamos características conv\n",
    "            feats = model(frames)\n",
    "            # restauramos lote y secuencia\n",
    "            feats = feats.reshape(b, s, -1)\n",
    "            # movemos a cpu y numpy\n",
    "            feats = feats.cpu().numpy()\n",
    "\n",
    "            # guardamos\n",
    "            for subpath, x, y in zip(subpaths, feats, ys):\n",
    "                # creamos arreglo\n",
    "                arr = z.create_dataset(subpath, data=x, dtype=np.float32)\n",
    "                # asignamos etiqueta\n",
    "                arr.attrs['y'] = y.item()\n",
    "\n",
    "else:\n",
    "    print(f'Características ya extraidas en {zarr_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 richardt richardt  36K Nov 16 14:56 ../data/ucf11/ucf11.zarr/basketball-v_shooting_01_01/0.0\n",
      "-rw-rw-r-- 1 richardt richardt  37K Nov 16 14:56 ../data/ucf11/ucf11.zarr/basketball-v_shooting_01_02/0.0\n",
      "-rw-rw-r-- 1 richardt richardt  37K Nov 16 14:56 ../data/ucf11/ucf11.zarr/basketball-v_shooting_01_03/0.0\n",
      "-rw-rw-r-- 1 richardt richardt  37K Nov 16 14:56 ../data/ucf11/ucf11.zarr/basketball-v_shooting_01_04/0.0\n",
      "-rw-rw-r-- 1 richardt richardt  37K Nov 16 14:56 ../data/ucf11/ucf11.zarr/basketball-v_shooting_01_05/0.0\n",
      "ls: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/ucf11/ucf11.zarr/*/* -hl | head -5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
