{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gibranfp/CursoAprendizajeProfundo/blob/2022-1/notebooks/4a_ucf11_feats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de características convolucionales de cuadros de video\n",
    "\n",
    "Curso: [Aprendizaje Profundo](http://turing.iimas.unam.mx/~gibranfp/cursos/aprendizaje_profundo/). Profesor: [Gibran Fuentes Pineda](http://turing.iimas.unam.mx/~gibranfp/). Ayudantes: [Bere](https://turing.iimas.unam.mx/~bereml/) y [Ricardo](https://turing.iimas.unam.mx/~ricardoml/).\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "En esta libreta usaremos un modelo CNN preentreando como extractor de características convolucionales de cuadros del conjunto [UCF11](https://www.crcv.ucf.edu/data/UCF_YouTube_Action.php).\n",
    "\n",
    "<img src=\"https://www.cs.ucf.edu/~liujg/realistic_actions/youtube_snaps.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ny0L2LzogTN-"
   },
   "outputs": [],
   "source": [
    "# sistema de archivos\n",
    "import os\n",
    "# listar archivos por patrón\n",
    "from glob import glob\n",
    "# sistema de archivos\n",
    "from os.path import join\n",
    "# flush!\n",
    "import sys\n",
    "\n",
    "# arreglos multidimensionales\n",
    "import numpy as np\n",
    "# redes neuronales\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as tvm\n",
    "import torchvision.transforms as T\n",
    "# almacenamiento de arreglos multidimensionales\n",
    "import zarr\n",
    "# redes\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import read_video\n",
    "# barras de progreso\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "BATCH_SIZE = 5\n",
    "# numéro de cuadros por video\n",
    "NUM_FRAMES = 10\n",
    "IMG_SIZE = 224\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCF11:\n",
    "\n",
    "    def __init__(self, videos_dir, num_frames, tsfm):\n",
    "        pattern = join(videos_dir, '*', '*', '*.mpg')\n",
    "        self.paths = sorted(glob(pattern))\n",
    "        self.num_frames = num_frames\n",
    "        self.tsfm = tsfm\n",
    "        # removemos videos demasiado cortos\n",
    "        self._filter_out_videos()        \n",
    "        # UCF11 classes\n",
    "        classes = (\n",
    "            'basketball', 'biking', 'diving', 'golf_swing', \n",
    "            'horse_riding', 'soccer_juggling', 'swing', 'tennis_swing',\n",
    "            'trampoline_jumping', 'volleyball_spiking', 'walking'\n",
    "        )\n",
    "        self.cls_idx = {c: i for i, c in enumerate(classes)}\n",
    "    \n",
    "    def _filter_out_videos(self):\n",
    "        \"\"\"Remueve videos con menos de `num_frames` frames.\"\"\"\n",
    "        print(f'Removiendo videos con menos de {self.num_frames} cuadros')\n",
    "        sys.stdout.flush()\n",
    "        too_short = []\n",
    "        for path in tqdm(self.paths):\n",
    "            frames = read_video(path, pts_unit='sec')[0].shape[0]\n",
    "            if frames < self.num_frames:\n",
    "                too_short.append(path)\n",
    "        for path in too_short:\n",
    "            self.paths.remove(path)\n",
    "            \n",
    "    def __getitem__(self, i):\n",
    "        path = self.paths[i]\n",
    "        \n",
    "        # ubtenemos subruta class/group/video.mpg\n",
    "        parts = path.split('/')[4:]\n",
    "        subpath = '-'.join([parts[0], parts[2]])[:-4]\n",
    "        \n",
    "        # leemos el video completo\n",
    "        frames = read_video(path, pts_unit='sec')[0]\n",
    "        # calculamos el salto\n",
    "        step = frames.shape[0] // self.num_frames\n",
    "        # creamos indices saltando\n",
    "        indices = np.arange(0, step * self.num_frames, step)\n",
    "        # calculamos los indices restantes y dividismo entre 2\n",
    "        offset = (frames.shape[0] - indices[-1]) // 2\n",
    "        # recorremos a la derecha para centrar\n",
    "        indices += offset\n",
    "        # seleccionamos los cuadros\n",
    "        frames = frames[indices] \n",
    "        # convert to channel first\n",
    "        frames = frames.movedim(3, 1)\n",
    "        # aplicamos trasformación\n",
    "        frames = self.tsfm(frames)\n",
    "        \n",
    "        # obtenemos etiqueta\n",
    "        y = self.cls_idx[parts[0]]\n",
    "\n",
    "        return subpath, frames, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "\n",
    "tsfm = T.Compose([\n",
    "    # redimensionamos a 224x224\n",
    "    T.Resize(IMG_SIZE),\n",
    "    # cortamos al centro\n",
    "    T.CenterCrop(IMG_SIZE),\n",
    "    # uint => float, x ∈ [0, 1]\n",
    "    T.ConvertImageDtype(torch.float),\n",
    "    # estandarizamos con media y desviación estandar\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Extracción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se asume que [UCF11](https://www.crcv.ucf.edu/data/UCF11_updated_mpg.rar) se descargó y extrajó en `DATA_DIR/ucf11`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando ../data/ucf11/UCF11_updated_mpg\n",
      "Removiendo videos con menos de 10 cuadros\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd4ba63fbca4aed8f27bc91d4ec958b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo características en ../data/ucf11/ucf11.zarr\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045c97ef41b6498782bbe3639bdede27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/richardt/.miniconda3/envs/cap/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "data_dir = join(DATA_DIR, 'ucf11')\n",
    "zarr_dir = join(data_dir, 'ucf11.zarr')\n",
    "\n",
    "if not os.path.isdir(zarr_dir):\n",
    "    \n",
    "    videos_dir = join(data_dir, 'UCF11_updated_mpg')\n",
    "    print(f'Usando {videos_dir}')\n",
    "\n",
    "    ds = UCF11(videos_dir, NUM_FRAMES, tsfm)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SIZE, num_workers=4)\n",
    "\n",
    "    print(f'Extrayendo características en {zarr_dir}')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = tvm.densenet121(pretrained=True)\n",
    "    model.classifier = nn.Identity()\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    z = zarr.open(zarr_dir, 'w')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for subpaths, frames, ys in tqdm(dl):\n",
    "\n",
    "            # datos a dispositivo\n",
    "            frames = frames.to(device)\n",
    "\n",
    "            # guardamos dimensiones\n",
    "            b, s, *img = frames.shape\n",
    "            # planamos lote y secuencia en una sola dimensión\n",
    "            frames = frames.reshape(-1, *img)\n",
    "            # computamos características conv\n",
    "            feats = model(frames)\n",
    "            # restauramos lote y secuencia\n",
    "            feats = feats.reshape(b, s, -1)\n",
    "            # movemos a cpu y numpy\n",
    "            feats = feats.cpu().numpy()\n",
    "\n",
    "            # guardamos\n",
    "            for subpath, x, y in zip(subpaths, feats, ys):\n",
    "                # creamos arreglo\n",
    "                arr = z.create_dataset(subpath, data=x, dtype=np.float32)\n",
    "                # asignamos etiqueta\n",
    "                arr.attrs['y'] = y.item()\n",
    "\n",
    "else:\n",
    "    print(f'Características ya extraidas en {zarr_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 richardt richardt 36K Oct 27 23:02 ../data/ucf11/ucf11.zarr/basketball-v_shooting_01_01/0.0\n",
      "-rw-rw-r-- 1 richardt richardt 36K Oct 27 23:02 ../data/ucf11/ucf11.zarr/basketball-v_shooting_01_02/0.0\n",
      "-rw-rw-r-- 1 richardt richardt 36K Oct 27 23:02 ../data/ucf11/ucf11.zarr/basketball-v_shooting_01_03/0.0\n",
      "-rw-rw-r-- 1 richardt richardt 37K Oct 27 23:02 ../data/ucf11/ucf11.zarr/basketball-v_shooting_01_04/0.0\n",
      "-rw-rw-r-- 1 richardt richardt 36K Oct 27 23:02 ../data/ucf11/ucf11.zarr/basketball-v_shooting_01_05/0.0\n",
      "ls: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!ls {zarr_dir}/*/* -lh | head -5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
