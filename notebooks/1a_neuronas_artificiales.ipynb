{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gibranfp/CursoAprendizajeProfundo/blob/2025-1/notebooks/1a_neuronas_artificiales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8X1X5KGWhZT"
      },
      "source": [
        "# Neuronas artificiales\n",
        "---\n",
        "La neurona artificial es un modelo simplificado de la neurona natural, la cual trata de imitar 3 aspectos principales:\n",
        "\n",
        "1. La fuerza sináptica que pondera los impulsos recibidos\n",
        "2. La acumulación de estos impulsos ponderados\n",
        "3. La activación de la neurona que produce un impulso de respuesta a su salida.\n",
        "\n",
        "La primera neurona artificial fue la llamada Unidad de Umbral Lineal (LTU por las siglas en inglés de _Linear Threshold Unit_), propuesta en 1943 por Warren McCulloch y Walter Pitts, la cual presupone que tanto los valores de los atributos de entrada como los valores de salida son binarios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4qjFKtZafzQ"
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ec2lh4iaj7J"
      },
      "source": [
        "## Unidad de Umbral Lineal\n",
        "La operación que lleva a cabo una neurona articial está dada por la suma pesada evaluada con una función de activación $\\phi$. Una de las primeras funciones de activación utilizadas fue la escalón unitario, definida como\n",
        "\n",
        "$\n",
        "\\phi(x) = \\begin{cases} 1, & \\text{si } x \\geq 0\\\\0, & \\text{en caso contrario}\\end{cases}\n",
        "$\n",
        "\n",
        "Esta se puede llevar a cabo con la siguiente función de Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DF3X4nFS8ze"
      },
      "source": [
        "def escalon(z):\n",
        "  if z >= 0.0:\n",
        "    return 1.0\n",
        "  else:\n",
        "    return 0.0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHUpzFQLY8Y1"
      },
      "source": [
        "Por su parte, la suma pesada simplemente consiste en multiplicar cada entrada por su correspondiente peso y sumarle el sesgo. Esto lo podemos expresar como\n",
        "\n",
        "$\n",
        "z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\cdots + w_d \\cdot x_d + b\n",
        "$\n",
        "\n",
        "En su forma vectorial\n",
        "\n",
        "$\n",
        "z = \\mathbf{w}^T \\mathbf{x} + b\n",
        "$\n",
        "\n",
        "Para realizar esto en Python, podemos usar la función `dot` de NumPY de la siguiente manera `z = np.dot(w.T, x) + b`. Así, la operación de la neurona completa sería:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G61wzAPaT3r3"
      },
      "source": [
        "def neurona(x, w, b):\n",
        "  z = np.dot(w.T, x) + b\n",
        "  return escalon(z)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-VjUcwrZpmW"
      },
      "source": [
        "### OR ($\\lor$)\n",
        "Esta neurona es capaz de aproximar la compuerta OR, cuya salida es 1 cuando al menos 1 de las 2 entradas es 1:\n",
        "\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ------------- |:-------------:| -----:|\n",
        "|0 |0 |0|\n",
        "|0 |1 |1|\n",
        "|1 |0 |1|\n",
        "|1 |1 |1|\n",
        "\n",
        "La neurona recibe 2 valores binarios como entrada y produce un valor binario como salida. Específicamente, la neurona calcularía\n",
        "\n",
        "$\n",
        "\\hat{y} = \\phi(w_1 \\cdot x_1 + w_2 \\cdot x_2 + b)\n",
        "$\n",
        "\n",
        "Para poder la compuerta OR es necesario encontrar los valores apropiados de $w_1$, $w_2$ y $b$. Una posible elección sería 10, 10 y -5 respectivamente. Verifiquemos estos valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE5LjZh9TWcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c24c9b-f497-466b-c5e5-4ad2e511acb5"
      },
      "source": [
        "X = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
        "w = np.array([10, 10])\n",
        "b = -5\n",
        "\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\ty_hat')\n",
        "print('-----------------------------')\n",
        "for i in range(X.shape[0]):\n",
        "  y_hat = neurona(X[i, :].T, w, b)\n",
        "  print('{0} \\t{1}\\t{2}'.format(X[i, 0], X[i, 1], y_hat))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "x_1 \tx_2 \ty_hat\n",
            "-----------------------------\n",
            "0.0 \t0.0\t0.0\n",
            "0.0 \t1.0\t1.0\n",
            "1.0 \t0.0\t1.0\n",
            "1.0 \t1.0\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnqeydbxeavD"
      },
      "source": [
        "### AND ($\\land$)\n",
        "De forma similar, podemos aproximar la computerta AND:\n",
        "\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ------------- |:-------------:| -----:|\n",
        "|0 |0 |0|\n",
        "|0 |1 |0|\n",
        "|1 |0 |0|\n",
        "|1 |1 |1|\n",
        "\n",
        "Nuevamente, debemos encontrar los valores apropiados para los pesos y el sesgo. Probemos con $w_1 = 10$, $w_2 = 10$ y $b = -15$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdcp_-oqTc75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf130602-b4cd-44b3-87e8-5873b92993aa"
      },
      "source": [
        "w = np.array([10, 10])\n",
        "b = -15\n",
        "\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\ty_hat')\n",
        "print('-----------------------------')\n",
        "for i in range(X.shape[0]):\n",
        "  y_hat = neurona(X[i, :].T, w, b)\n",
        "  print('{0} \\t{1}\\t{2}'.format(X[i, 0], X[i, 1], y_hat))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "x_1 \tx_2 \ty_hat\n",
            "-----------------------------\n",
            "0.0 \t0.0\t0.0\n",
            "0.0 \t1.0\t0.0\n",
            "1.0 \t0.0\t0.0\n",
            "1.0 \t1.0\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJW4l3Gcx-j2"
      },
      "source": [
        "### NAND\n",
        "También podemos aproximar la computerta NAND:\n",
        "\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ------------- |:-------------:| -----:|\n",
        "|0 |0 |1|\n",
        "|0 |1 |1|\n",
        "|1 |0 |1|\n",
        "|1 |1 |0|\n",
        "\n",
        "En este caso, los valores para los pesos y el sesgo son $w_1 = -10$, $w_2 = -10$ y $b = 15$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJoT4krVyGEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945d3516-191b-47ba-c73e-39d9cbfea0d8"
      },
      "source": [
        "w = np.array([-10, -10])\n",
        "b = 15\n",
        "\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\ty_hat')\n",
        "print('-----------------------------')\n",
        "for i in range(X.shape[0]):\n",
        "  y_hat = neurona(X[i, :].T, w, b)\n",
        "  print('{0} \\t{1}\\t{2}'.format(X[i, 0], X[i, 1], y_hat))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "x_1 \tx_2 \ty_hat\n",
            "-----------------------------\n",
            "0.0 \t0.0\t1.0\n",
            "0.0 \t1.0\t1.0\n",
            "1.0 \t0.0\t1.0\n",
            "1.0 \t1.0\t0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqXVXWhve5pR"
      },
      "source": [
        "## Algoritmo Perceptrón\n",
        "El Perceptron es un algoritmo para entrenar neuronas artificiales  propuesto por Frank Rosenblatt en 1958. El procedimiento general es el siguiente:\n",
        "\n",
        "1. En $t=0$, inicializa los pesos $\\mathbf{w}^{[0]}$ y el sesgo $b^{[0]}$ con ceros o números aleatorios pequeños\n",
        "2. Para cada ejemplo $(\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)})$ en el conjunto de entrenamiento:\n",
        "\n",
        "    a. Calcula la salida:\n",
        "\n",
        "    $$\\hat{y}^{(i)} = \\phi(\\mathbf{w}^{[t]\\top} \\mathbf{x}^{(i)} + b^{[t]})$$\n",
        "\n",
        "    b. Actualiza cada peso $w_j, j = 1, \\ldots, d$ y el sesgo $b$:\n",
        "\n",
        "\\begin{align*}\n",
        "w_j^{[t + 1]} = & w_j^{[t]} + (y^{(i)} - \\hat{y}^{(i)})\\cdot x^{(i)}_j\\\\\n",
        "b^{[t + 1]} = & b^{[t]} + (y^{(i)} - \\hat{y}^{(i)})\n",
        "\\end{align*}\n",
        "\n",
        "3. Repite 2 hasta que se cumpla algún criterio de convergencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7kivF042Heh"
      },
      "source": [
        "def perceptron(X, y, n_epochs = 10):\n",
        "  w = np.zeros(X.shape[1])\n",
        "  b = 0\n",
        "  for i in range(n_epochs):\n",
        "    serr = 0.0\n",
        "    for j in range(X.shape[0]):\n",
        "      y_hat = neurona(X[j].T, w, b)\n",
        "      error = y[j] - y_hat\n",
        "\n",
        "      w += error * X[j].T\n",
        "      b += error\n",
        "\n",
        "      serr += np.abs(error)\n",
        "\n",
        "    print(\"Epoch {0}: error = {1}\".format(i, serr / float(X.shape[0])))\n",
        "\n",
        "  return w, b"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8WH4x7afCKv"
      },
      "source": [
        "### Aprendiendo la compuerta OR\n",
        "Probemos el algoritmo del perceptrón para aprender la computerta OR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOFZdaie9D6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560f206f-433f-462d-9662-3768a3de6702"
      },
      "source": [
        "X = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
        "y_or = np.array([0., 1., 1., 1.])\n",
        "\n",
        "w, b = perceptron(X, y_or)\n",
        "\n",
        "print('\\nw_1 = {0}, w_2 = {1}, b = {2}'.format(w[0], w[1], b))\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\t y\\ty_hat')\n",
        "print('-----------------------------')\n",
        "for i in range(X.shape[0]):\n",
        "  y_hat = neurona(X[i].T, w, b)\n",
        "  print('{0}\\t{1}\\t{2}\\t{3}'.format(X[i, 0], X[i, 1], y_or[i], y_hat))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: error = 0.25\n",
            "Epoch 1: error = 0.5\n",
            "Epoch 2: error = 0.25\n",
            "Epoch 3: error = 0.0\n",
            "Epoch 4: error = 0.0\n",
            "Epoch 5: error = 0.0\n",
            "Epoch 6: error = 0.0\n",
            "Epoch 7: error = 0.0\n",
            "Epoch 8: error = 0.0\n",
            "Epoch 9: error = 0.0\n",
            "\n",
            "w_1 = 1.0, w_2 = 1.0, b = 0.0\n",
            "-----------------------------\n",
            "x_1 \tx_2 \t y\ty_hat\n",
            "-----------------------------\n",
            "0.0\t0.0\t0.0\t0.0\n",
            "0.0\t1.0\t1.0\t1.0\n",
            "1.0\t0.0\t1.0\t1.0\n",
            "1.0\t1.0\t1.0\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSinG4G6rSCF"
      },
      "source": [
        "### Aprendiendo la operación AND\n",
        "Ahora veamos qué ocurre si en lugar de la OR aprendemos la compuerta AND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqyp83uorS_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f55b91b6-f941-4c13-dd77-62fbc62c19e1"
      },
      "source": [
        "y_and = np.array([0., 0., 0., 1.])\n",
        "w, b = perceptron(X, y_and)\n",
        "\n",
        "print('\\nw_1 = {0}, w_2 = {1}, b = {2}'.format(w[0], w[1], b))\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\t y\\ty_hat')\n",
        "print('-----------------------------')\n",
        "for i in range(X.shape[0]):\n",
        "  y_hat = neurona(X[i].T, w, b)\n",
        "  print('{0}\\t{1}\\t{2}\\t{3}'.format(X[i, 0], X[i, 1], y_and[i], y_hat))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: error = 0.25\n",
            "Epoch 1: error = 0.75\n",
            "Epoch 2: error = 0.75\n",
            "Epoch 3: error = 0.5\n",
            "Epoch 4: error = 0.25\n",
            "Epoch 5: error = 0.0\n",
            "Epoch 6: error = 0.0\n",
            "Epoch 7: error = 0.0\n",
            "Epoch 8: error = 0.0\n",
            "Epoch 9: error = 0.0\n",
            "\n",
            "w_1 = 2.0, w_2 = 1.0, b = -2.0\n",
            "-----------------------------\n",
            "x_1 \tx_2 \t y\ty_hat\n",
            "-----------------------------\n",
            "0.0\t0.0\t0.0\t0.0\n",
            "0.0\t1.0\t0.0\t0.0\n",
            "1.0\t0.0\t0.0\t0.0\n",
            "1.0\t1.0\t1.0\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c406Vb4Rz2bm"
      },
      "source": [
        "## Aproximando funciones no lineales: XOR ($\\oplus$)\n",
        "Minsky y Papert mostraron que una neurona del tipo LTU no puede aproximar de forma precisa una función no lineal como la compuerta XOR ($\\oplus$):\n",
        "\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ------------- |:-------------:| -----:|\n",
        "|0 |0 |0|\n",
        "|0 |1 |1|\n",
        "|1 |0 |1|\n",
        "|1 |1 |0|\n",
        "\n",
        "Sin embargo, es posible aproximarlas combinando múltiples neuronas LTU conectadas en red. Por ejemplo, es posible llevar a cabo la compuerta XOR con compuertas OR, AND y NAND:\n",
        "\n",
        "$\n",
        "\t  x_1 \\mathbin{\\oplus} x_2 = (x_1 \\lor x_2) \\land \\neg(x_1 \\land x_2)\n",
        "\t$  \n",
        "\n",
        "Esto lo llevamos a cabo con la siguiente función:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucf4fSl01XTA"
      },
      "source": [
        "def multicapa(x, W1, b1, W2, b2):\n",
        "  escv = np.vectorize(escalon)\n",
        "  a = escv(np.dot(W1.T, x) + b1)\n",
        "  return escv(np.dot(W2.T, a) + b2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbStOf7Qif1P"
      },
      "source": [
        "Encontrando los valores de pesos y sesgos adecuados, podemos usar esta función para aproximar la computerta XOR. Ya hemos encontrado los pesos y sesgos para las compuertas OR, AND y NAND, por lo que podemos usar estas neuronas con sus correspondientes pesos y sesgos. La red tendría 2 neuronas conectadas a las entradas que realizan las operaciones OR ($w_{11}^{\\{1\\}} = 10$, $w_{12}^{\\{1\\}} = 10$ y $b_1^{\\{1\\}} = -5$)  y NAND ($w_{21}^{\\{1\\}} = -10$, $w_{22}^{\\{1\\}} = -10$ y $b_2^{\\{1\\}} = 15$) respectivamente. La salida de estas 2 neuronas estarían conectadas a una tercera neurona que realiza la operacioón AND ($w_{11}^{\\{2\\}} = 10$, $w_{12}^{\\{2\\}} = 10$ y $b_1^{\\{2\\}} = -15$). En su forma matricial:\n",
        "\n",
        "$$\n",
        "\\mathbf{W}^{\\{1\\}} = \\left[\\begin{matrix}\n",
        "10 & -10\\\\\n",
        "10 & -10\n",
        "\\end{matrix}\\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{b}^{\\{1\\}} = \\left[\\begin{matrix}\n",
        "-5 \\\\\n",
        "15\n",
        "\\end{matrix}\\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{W}^{\\{2\\}} = \\left[\\begin{matrix}\n",
        "10\\\\\n",
        "10\n",
        "\\end{matrix}\\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{b}^{\\{2\\}} = \\left[\\begin{matrix}\n",
        "-15\\\\\n",
        "\\end{matrix}\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OtTynPBsPax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c879b112-403c-4b87-d39b-7e02c6ac3026"
      },
      "source": [
        "y_xor = np.array([0., 1., 1., 0.])\n",
        "W1 = np.array([[10, -10], [10, -10]])\n",
        "b1 = np.array([-5, 15])\n",
        "\n",
        "W2 = np.array([[10], [10]])\n",
        "b2 = np.array([-15])\n",
        "\n",
        "print('W_1 = [{0}{1}], b_1 = {2}'.format(W1[0, :], W1[1, :], b1))\n",
        "print('W_2 = [{0}{1}], b_2 = {2}'.format(W2[0], W2[1], b2))\n",
        "print('-----------------------------')\n",
        "print('x_1 \\tx_2 \\ty\\ty_hat')\n",
        "print('-----------------------------')\n",
        "for i in range(X.shape[0]):\n",
        "  y_hat = multicapa(X[i].T, W1, b1, W2, b2)\n",
        "  print('{0}\\t{1}\\t{2}\\t{3}'.format(X[i, 0], X[i, 1], y_xor[i], y_hat[0]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_1 = [[ 10 -10][ 10 -10]], b_1 = [-5 15]\n",
            "W_2 = [[10][10]], b_2 = [-15]\n",
            "-----------------------------\n",
            "x_1 \tx_2 \ty\ty_hat\n",
            "-----------------------------\n",
            "0.0\t0.0\t0.0\t0.0\n",
            "0.0\t1.0\t1.0\t1.0\n",
            "1.0\t0.0\t1.0\t1.0\n",
            "1.0\t1.0\t0.0\t0.0\n"
          ]
        }
      ]
    }
  ]
}