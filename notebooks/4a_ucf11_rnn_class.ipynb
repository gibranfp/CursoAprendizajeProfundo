{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gibranfp/CursoAprendizajeProfundo/blob/2022-1/notebooks/4a_ucf11_rnn_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de acciones humanas usando RNNs \n",
    "\n",
    "Curso: [Aprendizaje Profundo](http://turing.iimas.unam.mx/~gibranfp/cursos/aprendizaje_profundo/). Profesor: [Gibran Fuentes Pineda](http://turing.iimas.unam.mx/~gibranfp/). Ayudantes: [Bere](https://turing.iimas.unam.mx/~bereml/) y [Ricardo](https://turing.iimas.unam.mx/~ricardoml/).\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "En esta libreta entrenaremos un modelo basado en RNNs para reconocimiento de acciones humanas (HAR) en el conjunto [UCF11](https://www.crcv.ucf.edu/data/UCF_YouTube_Action.php).\n",
    "\n",
    "<img src=\"https://www.cs.ucf.edu/~liujg/realistic_actions/youtube_snaps.jpg\" />\n",
    "\n",
    "Este ejemplo está basado en las ideas presentadas en [*Long-term Recurrent Convolutional Networks for Visual Recognition and Description*](https://arxiv.org/abs/1411.4389) de 2016 por Donahue et al. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Preparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /home/richardt/.miniconda3/envs/cap/lib/python3.8/site-packages (1.5.3)\n",
      "Requirement already satisfied: zarr in /home/richardt/.miniconda3/envs/cap/lib/python3.8/site-packages (2.10.1)\n",
      "Requirement already satisfied: numpy>=1.7 in /home/richardt/.miniconda3/envs/cap/lib/python3.8/site-packages (from zarr) (1.20.3)\n",
      "Requirement already satisfied: asciitree in /home/richardt/.miniconda3/envs/cap/lib/python3.8/site-packages (from zarr) (0.3.3)\n",
      "Requirement already satisfied: numcodecs>=0.6.4 in /home/richardt/.miniconda3/envs/cap/lib/python3.8/site-packages (from zarr) (0.9.1)\n",
      "Requirement already satisfied: fasteners in /home/richardt/.miniconda3/envs/cap/lib/python3.8/site-packages (from zarr) (0.16)\n",
      "Requirement already satisfied: six in /home/richardt/.miniconda3/envs/cap/lib/python3.8/site-packages (from fasteners->zarr) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Colab\n",
    "# https://github.com/TylerYep/torchinfo\n",
    "!pip install torchinfo\n",
    "# https://zarr.readthedocs.io/en/stable/\n",
    "!pip install zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ny0L2LzogTN-"
   },
   "outputs": [],
   "source": [
    "# sistema de archivos\n",
    "import os\n",
    "# funciones aleatorias\n",
    "import random\n",
    "# descomprimir\n",
    "import tarfile\n",
    "# sistema de archivos\n",
    "from os.path import join\n",
    "\n",
    "# arreglos multidimensionales\n",
    "import numpy as np\n",
    "# redes neuronales\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets.utils as tvu\n",
    "# almacenamiento de arreglos multidimensionales\n",
    "import zarr\n",
    "#redes\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# inspección de arquitectura\n",
    "from torchinfo import summary\n",
    "\n",
    "# directorio de datos\n",
    "DATA_DIR = '../data'\n",
    "\n",
    "# tamaño del lote\n",
    "BATCH_SIZE = 32\n",
    "# tamaño del vector de características\n",
    "FEAT_SIZE = 1024\n",
    "\n",
    "# reproducibilidad\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch_gen = torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCF11:\n",
    "\n",
    "    def __init__(self, root, download=False):\n",
    "        self.root = root\n",
    "        self.zarr_dir = join(root, 'ucf11.zarr')\n",
    "        if download:\n",
    "            self.download()\n",
    "        self.z = zarr.open(self.zarr_dir, 'r')\n",
    "        self.paths = list(self.z.array_keys())\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        arr = self.z[self.paths[i]]\n",
    "        x = np.array(arr)\n",
    "        y = np.array(arr.attrs['y'], dtype=np.int64)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def _check_integrity(self):\n",
    "        return os.path.isdir(self.zarr_dir)\n",
    "    \n",
    "    def _extract(self, root, filename):\n",
    "        tar = tarfile.open(join(root, filename), \"r:gz\")\n",
    "        tar.extractall(root)\n",
    "        tar.close()\n",
    "\n",
    "    def download(self):\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "        tvu.download_url(\n",
    "            url='https://cloud.xibalba.com.mx/s/apYrNA4iM4K65o7/download',\n",
    "            root=self.root,\n",
    "            filename='ucf11.zarr.tar.gz',\n",
    "            md5='c8a82454f9ec092d00bcd99c849e03fd'\n",
    "        )\n",
    "        self._extract(self.root, 'ucf11.zarr.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Instancia del conjunto y partición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://cloud.xibalba.com.mx/s/apYrNA4iM4K65o7/download to ../data/ucf11/ucf11.zarr.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefcc9e2c2d045c8b36bc7d1eb6969b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53436566 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape=(10, 1024) dtype=float32\n",
      "x [0][:5]=[0.00022111 0.00368518 0.00314753 0.00201778 0.09296297]\n",
      "y shape=() dtype=int64 0\n",
      "y 0\n"
     ]
    }
   ],
   "source": [
    "ds = UCF11(join(DATA_DIR, 'ucf11'), True)\n",
    "x, y = ds[0]\n",
    "print(f'x shape={x.shape} dtype={x.dtype}')\n",
    "print(f'x [0][:5]={x[0][:5]}')\n",
    "print(f'y shape={y.shape} dtype={y.dtype} {y}')\n",
    "print(f'y {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1279, 320)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_size = int(0.8 * len(ds))\n",
    "tst_size = len(ds) - trn_size\n",
    "trn_ds, tst_ds = random_split(ds, [trn_size, tst_size])\n",
    "len(trn_ds), len(tst_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Cargadores de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(\n",
    "    # conjunto\n",
    "    trn_ds,\n",
    "    # tamaño del lote\n",
    "    batch_size=BATCH_SIZE,\n",
    "    # desordenar\n",
    "    shuffle=True,\n",
    "    # procesos paralelos\n",
    "    num_workers=2\n",
    ")\n",
    "tst_dl = DataLoader(\n",
    "    # conjunto\n",
    "    tst_ds,\n",
    "    # tamaño del lote\n",
    "    batch_size=BATCH_SIZE,\n",
    "    # desordenar\n",
    "    shuffle=True,\n",
    "    # procesos paralelos\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape=torch.Size([32, 10, 1024]) dtype=torch.float32\n",
      "y shape=torch.Size([32]) dtype=torch.int64\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(trn_dl))\n",
    "print(f'x shape={x.shape} dtype={x.dtype}')\n",
    "print(f'y shape={y.shape} dtype={y.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Modelo\n",
    "\n",
    "<!-- Torchvision provee una familia de [modelos](https://pytorch.org/docs/1.6.0/torchvision/models.html#classification) preentrenados en ImageNet. Usaremos [Shufflenet V2](https://arxiv.org/abs/1807.11164), una arquitectura eficiente para clasificación de imágenes.  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Definición de arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=1024, hidden_size=128, num_classes=11):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(input_size)\n",
    "        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=1, batch_first=True)\n",
    "        self.cls = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Batch, Seq, Feats, Hidden\n",
    "        # [B, S, F] => [B, F, S]\n",
    "        x = x.movedim(1, 2)\n",
    "        # [B, F, S]\n",
    "        x = self.bn(x)\n",
    "        # [B, F, S] => [B, S, F]\n",
    "        x = x.movedim(1, 2)\n",
    "        # [B, S, F] => [B, S, H]\n",
    "        x, _ = self.rnn(x)\n",
    "        # [B, S, H] => [B, H]\n",
    "        # toma el último paso, participación 1\n",
    "        x = x[:, -1, :]\n",
    "        # [B, H] = [B, 11]\n",
    "        x = self.cls(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN().eval()\n",
    "model(torch.zeros(1, 10, 1024)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Inspección de arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "RNN                                      --                        --\n",
       "├─BatchNorm1d: 1-1                       [1, 1024, 10]             2,048\n",
       "├─GRU: 1-2                               [1, 10, 128]              443,136\n",
       "├─Linear: 1-3                            [1, 11]                   1,419\n",
       "==========================================================================================\n",
       "Total params: 446,603\n",
       "Trainable params: 446,603\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 4.43\n",
       "==========================================================================================\n",
       "Input size (MB): 0.04\n",
       "Forward/backward pass size (MB): 0.09\n",
       "Params size (MB): 1.79\n",
       "Estimated Total Size (MB): 1.92\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (1, 10, 1024), device='cpu', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Ciclo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E 0 loss=202.30 acc=32.19\n",
      "E 1 loss=178.21 acc=42.81\n",
      "E 2 loss=165.81 acc=46.56\n",
      "E 3 loss=158.54 acc=48.75\n",
      "E 4 loss=147.89 acc=51.25\n",
      "E 5 loss=147.74 acc=53.75\n",
      "E 6 loss=149.61 acc=51.88\n",
      "E 7 loss=144.33 acc=53.12\n",
      "E 8 loss=148.34 acc=55.31\n",
      "E 9 loss=145.79 acc=53.12\n"
     ]
    }
   ],
   "source": [
    "# optimizador\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ciclo de entrenamiento\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # modelo en modo de entrenamiento\n",
    "    model.train()\n",
    "    \n",
    "    # entrenamiento de una época\n",
    "    for x, y_true in trn_dl:\n",
    "        # hacemos inferencia para obtener los logits\n",
    "        y_lgts = model(x)\n",
    "        # calculamos la pérdida\n",
    "        loss = F.cross_entropy(y_lgts, y_true)\n",
    "        # vaciamos los gradientes\n",
    "        opt.zero_grad()\n",
    "        # retropropagamos\n",
    "        loss.backward()\n",
    "        # actulizamos parámetros\n",
    "        opt.step()\n",
    "\n",
    "    # desactivamos temporalmente la gráfica de cómputo\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # modelo en modo de evaluación\n",
    "        model.eval()\n",
    "        \n",
    "        losses, accs = [], []\n",
    "        # validación de la época\n",
    "        for x, y_true in tst_dl:\n",
    "            # hacemos inferencia para obtener los logits\n",
    "            y_lgts = model(x)\n",
    "            # calculamos las probabilidades\n",
    "            y_prob = F.softmax(y_lgts, 1)\n",
    "            # obtenemos la clase predicha\n",
    "            y_pred = torch.argmax(y_prob, 1)\n",
    "            \n",
    "            # calculamos la pérdida\n",
    "            loss = F.cross_entropy(y_lgts, y_true)\n",
    "            # calculamos la exactitud\n",
    "            acc = (y_true == y_pred).type(torch.float32).mean()\n",
    "\n",
    "            # guardamos históricos\n",
    "            losses.append(loss.item() * 100)\n",
    "            accs.append(acc.item() * 100)\n",
    "\n",
    "        # imprimimos métricas\n",
    "        loss = np.mean(losses)\n",
    "        acc = np.mean(accs)\n",
    "        print(f'E{epoch:2} loss={loss:6.2f} acc={acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participación 1\n",
    "\n",
    "Cambia la arquitectura para tomar el promedio de la secuencia de la última salida de la capa RNN en vez de tomar el último paso. Revisa la documentación de [`torch.mean`](https://pytorch.org/docs/stable/generated/torch.mean.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participación 2\n",
    "\n",
    "Remplaza la capa [GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html) por una [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
